{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"5-DiffE-RungeKutta.ipynb","version":"0.3.2","provenance":[],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"_5o1S6yXrpeR","colab_type":"text"},"cell_type":"markdown","source":["# <center>Solving Differential Equations</center>\n","## <center>Using PyTorch with Runge-Kutta</center>\n","### <center>Dr David Race</center>"]},{"metadata":{"id":"if8FECnSr8TW","colab_type":"text"},"cell_type":"markdown","source":["This notebook provides an introductory guide for using PyTorch to solve differential equations numerically.  This notebook uses implicit methods to solve a variety of DiffE problems using this Deep Learning toolkit.  At the conclusion of the notebook, you will see that PyTorch has these advantages:\n","\n","1.   Very pythonic\n","2.  Easily mixes with numpy and scipy\n","3.   It is a powerful numeric tool on GPUs (via Google's Colaboratory)\n","4.  It is easy to build custom classes and functions for numerical computing\n","5.  Relieves the tedium of performing basic gradient computations\n","\n","Overall this tool, while it is designed for Deep Learning, has a lot to offer for numerical computing.\n","\n"]},{"metadata":{"id":"0gCtFEllddD7","colab_type":"text"},"cell_type":"markdown","source":["##Background - Starting Point"]},{"metadata":{"id":"cxvRCxwgdgAa","colab_type":"text"},"cell_type":"markdown","source":["While solving differential equations, we seldom know an explicit function that satisfies the differential equation; therefore, we need to use numerical methods that rely on approximations for the derivatives.  One of the popular explicit methods is Runge-Kutta; therefore, this notebook shows how to use the Runge-Kutta method for better results.\n","\n","Using the implicit Runge-Kutta, we have the following:\n","\n","$y^{\\prime} = f(x,y)$, with $y(x_0) = y_0$.\n","\n","Then given a step size $h$, we iterate through until we reach the target value for x using this algorithm:\n","\n","For the target number of steps,\n","\n","Set $x=x_0, y = y_0$\n","\n","$k_1 = hf(x,y)$\n","\n","$k_2 = hf(x + \\frac{h}{2},y + \\frac{k_1}{2})$\n","\n","$k_3 = hf(x + \\frac{h}{2},y + \\frac{k_2}{2})$\n","\n","$k_4 = hf(x+h, y+k_3)$\n","\n","Set $x = x +h, y = y + \\frac{1}{6}(k1 + 2k_2 + 2k_3 + k_4)$\n"]},{"metadata":{"id":"WSjy6KyrAU65","colab_type":"text"},"cell_type":"markdown","source":["##Processing\n","\n","The processing will define a function to compute the y values and minimize the error.  In essence, if $F(x,y)$ is $f(x,y)$ computed in parallel, then we minimize the difference of $y^{\\prime} - F(x,y)$ and leverage PyTorch's autograd to compute the best fit for a particular interval choice.  The details are shown after the environment is established."]},{"metadata":{"id":"5lFCXIzDskcw","colab_type":"text"},"cell_type":"markdown","source":["## Set Up Environment"]},{"metadata":{"id":"U14UhNEbs8jn","colab_type":"text"},"cell_type":"markdown","source":["This section installs the basic pytorch capabilities into the Colaboratory VM."]},{"metadata":{"id":"zt7FWfbCtZP3","colab_type":"code","colab":{}},"cell_type":"code","source":["from os import path\n","from pprint import pprint, pformat\n","!pip3 install -U torch\n","import torch\n","accelerator = 'cuda' if torch.cuda.is_available() else 'cpu'\n","#Warning:  device is used as a global name so we don't have to pass around the accelerator device\n","global_device = torch.device(accelerator)\n","print(accelerator)\n","pprint(global_device)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VV0KYfWxiVj9","colab_type":"text"},"cell_type":"markdown","source":["Set up the numpy environment."]},{"metadata":{"id":"vOA2CK4Sriam","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","from numpy import linspace, exp\n","import torch"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IyI0AIYxiPnS","colab_type":"text"},"cell_type":"markdown","source":["Set up the graphics environment."]},{"metadata":{"id":"WbHqTkh5iZg_","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install --upgrade pandas\n","!pip show pandas\n","import pandas as pd\n","from pandas import DataFrame\n","from IPython.display import HTML, display\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","#the seaborn extension of matplotlib plots are generally \"prettier\"\n","import seaborn as sns\n","sns.set_style(\"white\")\n","\n","def pretty_display(x, y, x_label = 'X', y_label='Y', title='XY',num_show=25):\n","  max_show = min(len(x),num_show)\n","  x_vals = x[:max_show]\n","  y_vals = y[:max_show]\n","  xy = {x_label:x_vals, y_label:y_vals}\n","  df = DataFrame(xy)\n","  cols=[x_label, y_label]\n","  pData = DataFrame(df)\n","  pData = pData[cols]\n","  display(HTML('<b>'+title+'</b>'))\n","  display(HTML(pData.to_html()))\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zEUVPatZzZtf","colab_type":"text"},"cell_type":"markdown","source":["## Example 1 - \n","\n","$y^{\\prime} = y$, \n","\n","$y(0) = 1$ ,\n","\n","on the interval $[0,1]$\n","\n","As you know, this is simply $y=e^x$, but this is a traditional problem to start numerical studies due to its rapid growth rate and its familiarity in mathematics and computational sciences."]},{"metadata":{"id":"97YlXMz7tOkY","colab_type":"text"},"cell_type":"markdown","source":["###Investigations Into Implicit Runge-Kutta with PyTorch"]},{"metadata":{"id":"43X0AtraznP3","colab_type":"text"},"cell_type":"markdown","source":["In this particular case, we define two generic functions, compute lhs and loss"]},{"metadata":{"id":"3ayZdsiH37Sp","colab_type":"code","colab":{}},"cell_type":"code","source":["#Compute the left side of problem to solve\n","def compute_lhs(x, y, init_val):\n","  global global_device\n","  '''\n","  Purpose:  The function computes the approximate solution to y' = f(x,y).  Unlike\n","    the canonical explicit Runge-Kutta method, this computes all of the next Runge-Kutta\n","    values based upon the existing x and y values\n","  \n","  Required Definitions:  The function f(x,y) needs to be included in this function definition\n","  \n","  Inputs:\n","    x - a tensor with the current x values\n","    y - a tensor with the current y values (the y -values will be updated by the gradient steps)\n","    init_val - the initial value of y\n","    \n","  Outputs:\n","    The left hand side of the equation such that y' - f(x,y) = 0 and y[0] converges to the initial value\n","\n","    \n","  TBD:  Maybe consider passing in f(x,y)\n","  '''\n","  h = x[1] - x[0]  #At some point we need to compute these for each interval so we have variable spacing\n","  \n","#   def y_prime(y):\n","#     assert type(y) is torch.Tensor\n","#     return y\n","  \n","  def f(x,y):\n","    assert type(x) is torch.Tensor\n","    assert type(y) is torch.Tensor\n","    return y\n","  \n","  def k1(x,y,h):\n","    assert type(x) is torch.Tensor\n","    assert type(y) is torch.Tensor\n","    K1 = h * f(x,y)\n","    return K1\n","  \n","  def k2(x,y,h, k1):\n","    assert type(x) is torch.Tensor\n","    assert type(y) is torch.Tensor\n","    assert type(k1) is torch.Tensor\n","    K2 = h * f(x + h/2.0, y + k1/2.0)\n","    return K2\n","  \n","  def k3(x,y,h,k2):\n","    assert type(x) is torch.Tensor\n","    assert type(y) is torch.Tensor\n","    assert type(k2) is torch.Tensor\n","    K3 = h * f(x + h/2.0, y + k2/2.0)\n","    return K3\n","\n","  def k4(x,y,h, k3):\n","    assert type(x) is torch.Tensor\n","    assert type(y) is torch.Tensor\n","    assert type(k3) is torch.Tensor\n","    K4 = h * f(x+h, y + k3)\n","    return K4\n","  \n","  def F(x,y,h):\n","    assert type(x) is torch.Tensor\n","    assert type(y) is torch.Tensor\n","    \n","    K1 = k1(x,y,h)\n","    K2 = k2(x,y,h,K1)\n","    K3 = k3(x,y,h,K2)\n","    K4 = k4(x,y,h,K3)\n","    \n","    c = 1./6.\n","    Y = y + c*(K1 + 2.0 * K2 + 2.0 * K3 + K4)\n","    return Y\n","  '''\n","  Purpose:  Compute the left side of the equation to solve.  This implementaiton has the right side as zero.\n","  \n","  Inputs:\n","  x:  The x values on the interval of interest\n","  y:  The current approximation for the solution\n","  init_val:  The inital value for the left side of the interval\n","  h:  the uniform distance used in the solution\n","  \n","  Globals:\n","  global_device\n","  \n","  Output:\n","  c the result of the lhs computation of y' - F(x,y,h)\n","  '''\n","  \n","  N = y.size()[0]\n","  c = torch.zeros((N,1), device = global_device).double() #We generically use double precision\n","  c[0] = y[0] - init_val\n","  c[1:N] = y[1:N] - F(x,y,h)[0:(N-1)]\n","  return c\n","#\n","#  Compute the loss\n","#\n","def loss_xy(y, x, init_val):\n","  '''\n","  Purpose:  Compute the loss function that is used to compute the gradient for our solution to the DiffE\n","            NOTE:  Since the right side is computed to be zero, the lose is just the L2-norm squared\n","  Inputs:\n","  y:  The current estimate for the solution to the DiffE\n","  init_val:  The inital value for the left side of the interval\n","  h:  the uniform distance used in the solution\n","  \n","  Output:  The current loss value\n","  '''\n","  c = compute_lhs(x, y, init_val)\n","  e = torch.sum(c * c)\n","  return e\n","#\n","#  Watch for updates to these definitions as we progress through this notebook\n","#"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FNm6XmhPlVE1","colab_type":"code","colab":{}},"cell_type":"code","source":["class manage_grad_ema:\n","  global global_device\n","  '''\n","  Purpose:  Maintain the ema for the gradient and compute the running difference between the ema and the gradient.\n","  \n","  '''\n","  \n","  def __init__(self, ema_shape = None, num_average = 100.):\n","    '''\n","    Purpose:  initialize the gradient and the number of gradients to average\n","    Inputs:\n","    ema_shape - The shape of the gradient tensor\n","    num_average - The number of gradients to average using the exponential moving average\n","    device - the compute device for the tensor\n","    '''\n","    assert ema_shape is not None\n","    self.ema_shape = ema_shape\n","    self.device = global_device\n","    self.num_average = np.float(num_average)\n","    assert self.num_average > 0.0\n","    self.alpha = 2.0/(1.0 + self.num_average)\n","    self.ema = torch.zeros(self.ema_shape, device = self.device).double()\n","    self.diff_measure = 1.0\n","    \n","  def update(self, in_grad = None):\n","    '''\n","    Purpose:  Update the ema and compute the max abs of the current ema tensor and the current gradient\n","    \n","    Input\n","    in_grad - the current gradient\n","    \n","    Output\n","    The maximum of the difference between the current gradient and the current ema\n","    '''\n","    assert in_grad is not None\n","    assert type(in_grad) is torch.Tensor\n","    cur_grad_est = in_grad\n","    self.diff_measure = torch.max(torch.abs(self.ema - cur_grad_est))\n","    self.ema = cur_grad_est * self.alpha + self.ema * (1.0 - self.alpha)\n","    return self.diff_measure\n","  \n","  def get_diff_measure(self):\n","    return self.diff_measure\n","  def get_ema(self):\n","    return self.ema"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7kUANddFGOU_","colab_type":"code","colab":{}},"cell_type":"code","source":["#Simple example\n","start_x = 0.0\n","end_x = 1.0\n","N = 11\n","x = torch.tensor(linspace(start_x, end_x, N), device = global_device).double()\n","x = x.view((N,1))\n","y = torch.ones((N,1), device = global_device).double()\n","y[0,0]=1.0\n","y.requires_grad = True\n","pprint(x)\n","pprint(y)\n","h = (end_x - start_x)/np.float(N -1)\n","init_val = 1.0\n","\n","e = loss_xy(y,x,init_val)\n","pprint(e)\n","\n","optimizer1 = torch.optim.Adam([y], lr=.01)\n","i = 0\n","max_iter = 5000\n","epsilon = 1e-7\n","\n","current_ema = manage_grad_ema(y.size())\n","curr_diff = 1.0\n","\n","while i<=max_iter and curr_diff> epsilon:\n","  optimizer1.zero_grad()\n","  loss = loss_xy(y, x, init_val)\n","  loss.backward()\n","  optimizer1.step()\n","  curr_diff = current_ema.update(y.grad)\n","  if i % 1000 == 0:  \n","    print(\"Iteration {:d}  Loss {:.6e}  Current Diff {:.6e}\".format(i,loss.data, curr_diff))\n","  i = i + 1\n","\n","pprint(i)\n","pprint(\"Target Value of e is {:9f}\".format(np.exp(1.0)))\n","pprint(\"Approx for e is {:.9f}\".format(y[-1,0]))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"N8_RCUCF656r","colab_type":"text"},"cell_type":"markdown","source":["As you can see, even with this \"less than stellar\" initial value for the initial guess, the result is not very bad!  Now we construct an object to use mult-grid and include a better initialization."]},{"metadata":{"id":"LPCH4Ms8CJty","colab_type":"text"},"cell_type":"markdown","source":["### Building a Simple Library"]},{"metadata":{"id":"vXuk1v8uCQpR","colab_type":"text"},"cell_type":"markdown","source":["For the next step, we introduce better initial estimates and multi-grid methods as part of the study for these methods.  We will include all of the cells again just for clarity."]},{"metadata":{"id":"0d6kRPZ_Dazv","colab_type":"code","colab":{}},"cell_type":"code","source":["class manage_grad_ema:\n","  global global_device\n","  '''\n","  Purpose:  Maintain the ema for the gradient and compute the running difference between the ema and the gradient.\n","  \n","  '''\n","  \n","  def __init__(self, ema_shape = None, num_average = 100.):\n","    '''\n","    Purpose:  initialize the gradient and the number of gradients to average\n","    Inputs:\n","    ema_shape - The shape of the gradient tensor\n","    num_average - The number of gradients to average using the exponential moving average\n","    device - the compute device for the tensor\n","    '''\n","    assert ema_shape is not None\n","    self.ema_shape = ema_shape\n","    self.device = global_device\n","    self.num_average = np.float(num_average)\n","    assert self.num_average > 0.0\n","    self.alpha = 2.0/(1.0 + self.num_average)\n","    self.ema = torch.zeros(self.ema_shape, device = self.device).double()\n","    self.diff_measure = 1.0\n","    \n","  def update(self, in_grad = None):\n","    '''\n","    Purpose:  Update the ema and compute the max abs of the current ema tensor and the current gradient\n","    \n","    Input\n","    in_grad - the current gradient\n","    \n","    Output\n","    The maximum of the difference between the current gradient and the current ema\n","    '''\n","    assert in_grad is not None\n","    assert type(in_grad) is torch.Tensor\n","    cur_grad_est = in_grad\n","    self.diff_measure = torch.max(torch.abs(self.ema - cur_grad_est))\n","    self.ema = cur_grad_est * self.alpha + self.ema * (1.0 - self.alpha)\n","    return self.diff_measure\n","  \n","  def get_diff_measure(self):\n","    return self.diff_measure\n","  def get_ema(self):\n","    return self.ema"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4vgD8UbNDhks","colab_type":"code","colab":{}},"cell_type":"code","source":["#This function is passed to the solver\n","#Compute the left side of problem to solve\n","def compute_lhs(x, y, init_val):\n","  global global_device\n","  '''\n","  Purpose:  The function computes the approximate solution to y' = f(x,y).  Unlike\n","    the canonical explicit Runge-Kutta method, this computes all of the next Runge-Kutta\n","    values based upon the existing x and y values\n","  \n","  Required Definitions:  The function f(x,y) needs to be included in this function definition\n","  \n","  Inputs:\n","    x - a tensor with the current x values\n","    y - a tensor with the current y values (the y -values will be updated by the gradient steps)\n","    init_val - the initial value of y\n","    \n","  Outputs:\n","    The left hand side of the equation such that y' - f(x,y) = 0 and y[0] converges to the initial value\n","\n","    \n","  TBD:  Maybe consider passing in f(x,y)\n","  '''\n","  h = x[1] - x[0]  #At some point we need to compute these for each interval so we have variable spacing\n","  \n","#   def y_prime(y):\n","#     assert type(y) is torch.Tensor\n","#     return y\n","  \n","  def f(x,y):\n","    assert type(x) is torch.Tensor\n","    assert type(y) is torch.Tensor\n","    return y\n","  \n","  def k1(x,y,h):\n","    assert type(x) is torch.Tensor\n","    assert type(y) is torch.Tensor\n","    K1 = h * f(x,y)\n","    return K1\n","  \n","  def k2(x,y,h, k1):\n","    assert type(x) is torch.Tensor\n","    assert type(y) is torch.Tensor\n","    assert type(k1) is torch.Tensor\n","    K2 = h * f(x + h/2.0, y + k1/2.0)\n","    return K2\n","  \n","  def k3(x,y,h,k2):\n","    assert type(x) is torch.Tensor\n","    assert type(y) is torch.Tensor\n","    assert type(k2) is torch.Tensor\n","    K3 = h * f(x + h/2.0, y + k2/2.0)\n","    return K3\n","\n","  def k4(x,y,h, k3):\n","    assert type(x) is torch.Tensor\n","    assert type(y) is torch.Tensor\n","    assert type(k3) is torch.Tensor\n","    K4 = h * f(x+h, y + k3)\n","    return K4\n","  \n","  def F(x,y,h):\n","    assert type(x) is torch.Tensor\n","    assert type(y) is torch.Tensor\n","    \n","    K1 = k1(x,y,h)\n","    K2 = k2(x,y,h,K1)\n","    K3 = k3(x,y,h,K2)\n","    K4 = k4(x,y,h,K3)\n","    \n","    c = 1./6.\n","    Y = y + c*(K1 + 2.0 * K2 + 2.0 * K3 + K4)\n","    return Y\n","  '''\n","  Purpose:  Compute the left side of the equation to solve.  This implementaiton has the right side as zero.\n","  \n","  Inputs:\n","  x:  The x values on the interval of interest\n","  y:  The current approximation for the solution\n","  init_val:  The inital value for the left side of the interval\n","  h:  the uniform distance used in the solution\n","  \n","  Globals:\n","  global_device\n","  \n","  Output:\n","  c the result of the lhs computation of y' - F(x,y,h)\n","  '''\n","  \n","  N = y.size()[0]\n","  c = torch.zeros((N,1), device = global_device).double() #We generically use double precision\n","  c[0] = y[0] - init_val\n","  c[1:N] = y[1:N] - F(x,y,h)[0:(N-1)]\n","  return c\n","#\n","#  Compute the loss\n","#\n","def loss_xy(y = None, x = None, init_val = None, compute_lhs = None):\n","  '''\n","  Purpose:  Compute the loss function that is used to compute the gradient for our solution to the DiffE\n","            NOTE:  Since the right side is computed to be zero, the lose is just the L2-norm squared\n","  Inputs:\n","  y:  The current estimate for the solution to the DiffE\n","  x:  The evenly spaced x values\n","  init_val:  The inital value for the left side of the interval\n","  compute_lhs:  The function to compute the left hand side of the equation so that y' - f(x,y)  = 0\n","  \n","  Output:  The current loss value\n","  '''\n","  assert y is not None\n","  assert type(y) is torch.Tensor\n","  assert y.requires_grad is True\n","  assert x is not None\n","  assert type(x) is torch.Tensor\n","  assert init_val is not None\n","  assert compute_lhs is not None\n","  c = compute_lhs(x, y, init_val)\n","  e = torch.sum(c * c)\n","  return e\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ukc7cdWHmvKr","colab_type":"text"},"cell_type":"markdown","source":["### The Barzilai-Borwein Method"]},{"metadata":{"id":"cJHMHRamUuW3","colab_type":"text"},"cell_type":"markdown","source":["This section implements a solver that uses the Barzilai-Browein mthod.  Recall -\n","\n"]},{"metadata":{"id":"FCyWuys22E8_","colab_type":"text"},"cell_type":"markdown","source":["Without proof, we will use the <i>Barzilai-Borwein</i> method of computing a step size.  Using $\\nabla F$ to denote the gradient of $F$, then we use\n","\n","$x_{n+1} = x_{n} - \\gamma_n * \\nabla F(x_n)$, where\n","\n","$\\gamma_n = \\frac{(x_n - x_{n-1})^T [\\nabla F(x_n) - \\nabla F(x_{n-1}]}{\\left\\Vert \\nabla F(x_n) - \\nabla F(x_{n-1})\\right \\Vert_2^2}$\n","\n","This methodology is implemented in the next compute cells and augments the prvious steps for the basic solver."]},{"metadata":{"id":"iyicw4DRrfOO","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import torch\n","\n","class diffe_solver_bb:\n","  global global_device\n","  '''\n","  The first step in more generic numerical differential equation solvers\n","  Purpose:  Solve a numerical differential equation defined by the inputs\n","  \n","  Inputs:\n","    initial_guess: an num_samplesxNx1 matrix, default is None so will be computed based upon start_x, end_x and num_intervals\n","    num_samples:  The number of samples to use in the model\n","    start_x: the starting value of x\n","    end_x: the ending value of x\n","    num_intervals: the number of inform intervals in the interval [start_x, end_x]\n","    lhs_function: a function to compute the left hand side of a set of finite difference set of equations, the rhs is always assumed to be zero\n","    init_value:  the initial value for the Differential Equation\n","    max_iter:  the maximum number of iterations to execute\n","    max_grad:  the max gradient rate.  This generally uses the Barzilai-Borwein, but this sets a max value\n","    epsilon:  the value that provides the ema stopping point\n","    device:  This is PyTorch, so we want to use the GPU whenever possible.\n","    \n","  Outputs:\n","    i:  the number of iterations\n","    y:  the approximate solution to the difference equations\n","  '''\n","  def __init__(self, initial_guess = None, start_x = 0.0, end_x = 1.0,\n","              lhs_function = None, cost_function = None,\n","              init_value = None, max_iter = 50000, epsilon=1e-6, num_exp_average = 50,\n","              show_progress=True):\n","    \n","    assert type(initial_guess) is torch.Tensor\n","    assert initial_guess is not None\n","    assert start_x is not None\n","    assert end_x is not None\n","    assert lhs_function is not None\n","    assert cost_function is not None\n","    assert init_value is not None\n","    assert max_iter is not None\n","    assert epsilon is not None\n","    assert num_exp_average is not None\n","\n","    \n","    self.initial_guess = initial_guess\n","    self.device = global_device\n","    self.num_intervals = self.initial_guess.shape[0] - 1\n","    self.N = self.num_intervals + 1\n","    self.start_x = np.float(start_x)\n","    self.end_x = np.float(end_x)\n","    #Change to support x values in the def of y'\n","    self.x = torch.tensor(np.reshape(np.linspace(self.start_x, self.end_x, self.N),(self.N,1)), device = self.device).double()\n","    self.h = (self.end_x - self.start_x)/np.float(self.num_intervals)\n","    self.initial_guess = None\n","    if initial_guess is None:\n","      #This can be changes as desired to match the problem\n","      np.seed(0)\n","      self.initial_guess = torch.rand((self.N,1), device = self.device).double()\n","    else:\n","      self.initial_guess = torch.tensor(initial_guess, device = self.device).double()\n","    assert self.N == initial_guess.shape[0]\n","\n","    self.lhs_function = lhs_function\n","    self.loss = cost_function\n","    self.init_value = np.float(init_value)\n","    self.max_iter = int(max_iter)\n","    self.epsilon = np.float(epsilon)\n","    self.num_exp_average = np.float(num_exp_average)\n","    self.alpha = 2.0/(1.0 + self.num_exp_average)\n","\n","    self.show_progress = show_progress\n","    self.ema = manage_grad_ema(ema_shape = initial_guess.shape,num_average = self.num_exp_average)\n","    \n","  def _compute_lhs_(self, x, y):\n","    #need to update this when using x values\n","    lhs = self.lhs_function(x, y, self.init_value, self.h)\n","    return lhs\n","  \n","  def _stop_(self,g):\n","    c = self.ema.update(g)\n","    return c\n","  \n","  def _sample_mult_(self,a,b):\n","    r = torch.zeros([1], device = self.device).double()\n","    r = torch.mm(a,b)\n","    return r\n","  \n","  def _compute_step_size_(self, xn, del_xn, x_nm1, del_x_nm1):\n","    diff_del = del_xn - del_x_nm1\n","    denom = torch.sum(diff_del * diff_del)\n","    x_diff = xn - x_nm1\n","    x_diff_tp = torch.transpose(x_diff,0,1)\n","    num = self._sample_mult_(x_diff_tp, diff_del)\n","    step_size = num/denom\n","    #return step_size.clamp(0.0,self.max_grad)\n","    return step_size\n","    \n","  def solve(self):\n","\n","    y = self.initial_guess\n","    y.requires_grad = True\n","\n","    step_size = .01\n","#     loss = self._loss_xy_(y, self.x, self.init_value, self.h)\n","    loss = self.loss(y, self.x, self.init_value, self.lhs_function)\n","    loss.backward()\n","    grad_y_1 = y.grad\n","    \n","      \n","    y_1 = y.clone().to(self.device)\n","    \n","    with torch.no_grad():\n","      y = y - step_size * grad_y_1\n","      y.requires_grad = True\n","      \n","    i = 0\n","    cur_stop = 1.0\n","    while i<=self.max_iter and cur_stop > self.epsilon:\n","#       loss = self._loss_xy_(y, self.x, self.init_value, self.h)\n","      loss = self.loss(y, self.x, self.init_value, self.lhs_function)\n","      loss.backward()\n","      grad_y_new = y.grad.clone()\n","\n","      ss = self._compute_step_size_(y, grad_y_new, y_1, grad_y_1)\n","      y_1 = y.clone()\n","      grad_y_1 = grad_y_new.clone()\n"," \n","      with torch.no_grad():\n","        y = y - ss * grad_y_new\n","        y.requires_grad = True\n","\n","#       curr_loss = loss.data.numpy()\n","#       curr_loss_diff = np.abs(ema - curr_loss)\n","      cur_stop = self._stop_(y)\n","      if i % 500 == 0 and self.show_progress:\n","        pprint(\"*************\")\n","        pprint(self.epsilon)\n","        pprint(cur_stop)\n","        pprint(loss)\n","      i = i + 1\n","    return i, y, cur_stop"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VbXLAcqBHddE","colab_type":"text"},"cell_type":"markdown","source":["A simple test case."]},{"metadata":{"id":"OrRCEZdnHhQQ","colab_type":"code","colab":{}},"cell_type":"code","source":["start_x = 0.0\n","end_x = 1.0\n","num_intervals = 50\n","N = num_intervals + 1\n","init_value = 1.0\n","\n","init_guess = torch.ones((N,1), device = global_device).double()\n","\n","diff_solver =  diffe_solver_bb(initial_guess = init_guess, start_x = start_x, end_x = end_x,\n","              lhs_function = compute_lhs, cost_function = loss_xy,\n","              init_value = init_value, max_iter = 5000, epsilon=1e-6, num_exp_average = 50,\n","              show_progress=False)\n","\n","loops, y, cur_stop = diff_solver.solve()\n","pprint(y)\n","pprint(loops)\n","pprint(cur_stop)\n","pprint(\"Target Value of e is {:9f}\".format(exp(1.0)))\n","pprint(\"Approx for e is {:.9f}\".format(y[-1,0]))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jAyqzb7Kn2yT","colab_type":"text"},"cell_type":"markdown","source":["This works well, so we instantiate this into a multigrid method to converge to a stronger answer"]},{"metadata":{"id":"6W91lnHcNRUu","colab_type":"code","colab":{}},"cell_type":"code","source":["class diffe_solver_mm():\n","  global global_device\n","  '''\n","  The final step in more generic numerical differential equation solvers\n","  Purpose:  Solve a numerical differential equation.  This uses the diffe_solver to compute the individual number\n","    of intervals, but this one loops over multiple intervals with multiple samples.  The initial guess will\n","    just be random numbers for now!\n","  \n","  Inputs:\n","    start_x: the starting value of x\n","    end_x: the ending value of x\n","    num_start_intervals: the number of inform intervals in the interval [start_x, end_x]\n","    lhs_function: a function to compute the left hand side of a set of finite difference set of equations, the rhs is always assumed to be zero\n","    init_value:  the initial value for the Differential Equation\n","    epsilon:  the value that provides the ema stopping point\n","    device:  This is PyTorch, so we want to use the GPU whenever possible.\n","    \n","  Outputs:\n","    i:  the number of iterations\n","    y:  the approximate solution to the difference equations\n","  '''\n","  def __init__(self, start_x = 0.0, end_x = 1.0,\n","               num_start_intervals = 10, \n","               lhs_function = None, cost_function = None, init_value = None,  \n","               epsilon=1e-6, num_exp_average = 50,\n","               show_progress=False):\n","    assert start_x is not None\n","    assert end_x is not None\n","    assert num_start_intervals is not None\n","    assert lhs_function is not None\n","    assert cost_function is not None\n","    assert init_value is not None\n","    assert epsilon is not None\n","    assert num_exp_average is not None\n","    \n","    self.num_intervals = int(num_start_intervals)\n","    self.N = self.num_intervals + 1\n","    self.start_x = np.float(start_x)\n","    self.end_x = np.float(end_x)\n","    self.lhs_function = lhs_function\n","    self.cost_function = cost_function\n","    self.init_value = np.float(init_value)\n","    self.epsilon = np.float(epsilon)\n","    self.num_exp_average = np.float(num_exp_average)\n","    self.device = global_device\n","    self.show_progress = show_progress\n","    \n","  def iteration_error(self, prev_est, cur_est):\n","    '''\n","    It is important to notice that cur_est has twice the number of points as the prev_est\n","    thus the computation only returns the error estimate based upon the common x values\n","    '''\n","    assert type(prev_est) is torch.Tensor\n","    assert type(cur_est) is torch.Tensor\n","    assert 2 * prev_est.shape[0] - 1 == cur_est.shape[0]\n","    N = cur_est.shape[0]\n","    c = prev_est - cur_est[0:N:2,:]\n","    return torch.sum(c*c)\n","\n","  def init_guess_func(self, y):\n","    '''\n","    Purpose:  Once the first estimate is completed, double the number of intervals each time.\n","    Inputs\n","    y:  The current estimate(s)  since we have multiple samples\n","\n","    Outputs\n","    y_vals is the estimate for doubling the number of intervals\n","    '''\n","    assert type(y) is torch.Tensor\n","    N = y.shape[0]\n","    NN = 2 * N - 1\n","    y_vals = torch.ones((NN,1), device = self.device).double()\n","    y_vals[0:NN:2,0] =y[0:N,0]\n","    y_vals[1:NN-1:2,0] = (y_vals[0:(NN-1):2,0] + y_vals[2:NN:2,0])/2.0\n","    return y_vals\n","    \n","  def solve(self):\n","    n_int = self.num_intervals\n","    N = self.N\n","    rhs_ = self.init_value\n","    \n","    y = torch.rand((N,1), device = self.device).double()\n","    y.requires_grad = True\n","                                           \n","    iter_error = 1.0\n","    i = 0\n","    \n","    while iter_error > self.epsilon:\n","      y_old = y.clone().detach()\n","      y = self.init_guess_func(y_old)\n","      diff_solve = diffe_solver_bb(initial_guess = y,  \n","                             start_x = self.start_x, end_x = self.end_x,\n","                             lhs_function = self.lhs_function, cost_function = self.cost_function, init_value= rhs_, \n","                             epsilon=self.epsilon,show_progress = self.show_progress)\n","      n_iter, y, cur_stop = diff_solve.solve()\n","      iter_error = self.iteration_error(y_old, y)\n","      print(\"Number of iterations {:d} Number intervals {:d}  Current approx for right end point  {:.10f} Current stop {:.10f}\".format(n_iter, y.shape[0] -  1, y[-1,0], cur_stop))\n","      i += 1\n","      \n","    return y"],"execution_count":0,"outputs":[]},{"metadata":{"id":"S-6uADGutTlw","colab_type":"text"},"cell_type":"markdown","source":["Application of this to converge to an answer that is converges with epsilon between iterations."]},{"metadata":{"id":"SMWrzc46t1Hv","colab_type":"code","colab":{}},"cell_type":"code","source":["diff_solve_m = diffe_solver_mm(start_x = 0.0, end_x = 1.0, num_start_intervals = 10,\n","                             lhs_function = compute_lhs, cost_function = loss_xy, init_value = 1.0,\n","                             epsilon = 1e-10, num_exp_average = 50, show_progress = True)\n","\n","y = diff_solve_m.solve()\n","print(\"Target value for right end point {:.10f}\".format(np.exp(1.0)))\n","print(\"Current approx for right end point  {:.10f}\".format(y[-1,0]))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"46S0melS24_D","colab_type":"text"},"cell_type":"markdown","source":["This appears to be both fast and accurate, so let's move to another problem.  For different problems we only need to change the compute_lhs function to match the new f(x,y)"]},{"metadata":{"id":"eg7-M07YmLWT","colab_type":"text"},"cell_type":"markdown","source":["##Example Two - $y^{\\prime} = 2y$\n","\n","In this instance, we need to change the f(x,y) in the definition of compute_lhs.  Rather than pass this function into compute_lhs, the change is made in the next code cell then used in the following code cell.  Additionally, we will set the right end point to 2.0."]},{"metadata":{"id":"ZWEiiav_3lX4","colab_type":"code","colab":{}},"cell_type":"code","source":["#Compute the left side of problem to solve\n","def compute_lhs(x, y, init_val):\n","  global global_device\n","  '''\n","  Purpose:  The function computes the approximate solution to y' = f(x,y).  Unlike\n","    the canonical explicit Runge-Kutta method, this computes all of the next Runge-Kutta\n","    values based upon the existing x and y values\n","  \n","  Required Definitions:  The function f(x,y) needs to be included in this function definition\n","  \n","  Inputs:\n","    x - a tensor with the current x values\n","    y - a tensor with the current y values (the y -values will be updated by the gradient steps)\n","    init_val - the initial value of y\n","    \n","  Outputs:\n","    The left hand side of the equation such that y' - f(x,y) = 0 and y[0] converges to the initial value\n","\n","    \n","  TBD:  Maybe consider passing in f(x,y)\n","  '''\n","  h = x[1] - x[0]  #At some point we need to compute these for each interval so we have variable spacing\n","  \n","#   def y_prime(y):\n","#     assert type(y) is torch.Tensor\n","#     return y\n","  \n","  def f(x,y):\n","    assert type(x) is torch.Tensor\n","    assert type(y) is torch.Tensor\n","    return 2.0 * y\n","  \n","  def k1(x,y,h):\n","    assert type(x) is torch.Tensor\n","    assert type(y) is torch.Tensor\n","    K1 = h * f(x,y)\n","    return K1\n","  \n","  def k2(x,y,h, k1):\n","    assert type(x) is torch.Tensor\n","    assert type(y) is torch.Tensor\n","    assert type(k1) is torch.Tensor\n","    K2 = h * f(x + h/2.0, y + k1/2.0)\n","    return K2\n","  \n","  def k3(x,y,h,k2):\n","    assert type(x) is torch.Tensor\n","    assert type(y) is torch.Tensor\n","    assert type(k2) is torch.Tensor\n","    K3 = h * f(x + h/2.0, y + k2/2.0)\n","    return K3\n","\n","  def k4(x,y,h, k3):\n","    assert type(x) is torch.Tensor\n","    assert type(y) is torch.Tensor\n","    assert type(k3) is torch.Tensor\n","    K4 = h * f(x+h, y + k3)\n","    return K4\n","  \n","  def F(x,y,h):\n","    assert type(x) is torch.Tensor\n","    assert type(y) is torch.Tensor\n","    \n","    K1 = k1(x,y,h)\n","    K2 = k2(x,y,h,K1)\n","    K3 = k3(x,y,h,K2)\n","    K4 = k4(x,y,h,K3)\n","    \n","    c = 1./6.\n","    Y = y + c*(K1 + 2.0 * K2 + 2.0 * K3 + K4)\n","    return Y\n","  '''\n","  Purpose:  Compute the left side of the equation to solve.  This implementaiton has the right side as zero.\n","  \n","  Inputs:\n","  x:  The x values on the interval of interest\n","  y:  The current approximation for the solution\n","  init_val:  The inital value for the left side of the interval\n","  h:  the uniform distance used in the solution\n","  \n","  Globals:\n","  global_device\n","  \n","  Output:\n","  c the result of the lhs computation of y' - F(x,y,h)\n","  '''\n","  \n","  N = y.size()[0]\n","  c = torch.zeros((N,1), device = global_device).double() #We generically use double precision\n","  c[0] = y[0] - init_val\n","  c[1:N] = y[1:N] - F(x,y,h)[0:(N-1)]\n","  return c\n","#"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wpZ7YZ5X5U58","colab_type":"code","colab":{}},"cell_type":"code","source":["diff_solve_m = diffe_solver_mm(start_x = 0.0, end_x = 2.0, num_start_intervals = 10,\n","                             lhs_function = compute_lhs, cost_function = loss_xy, init_value = 1.0,\n","                             epsilon = 1e-7, num_exp_average = 50)\n","\n","y = diff_solve_m.solve()\n","print(\"Target value for right end point {:.10f}\".format(np.exp(4.0)))\n","print(\"Current approx for right end point  {:.10f}\".format(y[-1,0]))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CwnKVxGz-LQx","colab_type":"text"},"cell_type":"markdown","source":["Overall this is a fairly solid result for the amount of effort required to achieve the final answer.  A couple of the iterationcounts were fairly large, but the answeres settled in quickly."]},{"metadata":{"id":"Bi0QogBxpaXL","colab_type":"text"},"cell_type":"markdown","source":["##Example Three - $y^{\\prime} = x + 2y$"]},{"metadata":{"id":"OsNLrLMzIvW5","colab_type":"text"},"cell_type":"markdown","source":["Our problem for this section is:\n","\n","Solve -\n","\n","$y^{\\prime} = x + 2y$\n","\n","$y(0) = 0.25$\n","\n","on the interval $[0,2]$\n","\n","It can be shown that the soltuion for this is $y = \\frac{1}{2}\\Big(   e^{2x} - x -\\frac{1}{2} \\Big)$ so the value at $y(2) ~ 26.04880$.\n","\n","Once again, the changes are relati vely minor to \"solve\" this probelm."]},{"metadata":{"id":"edC_6WelGCzY","colab_type":"code","colab":{}},"cell_type":"code","source":["#Compute the left side of problem to solve\n","def compute_lhs(x, y, init_val):\n","  global global_device\n","  '''\n","  Purpose:  The function computes the approximate solution to y' = f(x,y).  Unlike\n","    the canonical explicit Runge-Kutta method, this computes all of the next Runge-Kutta\n","    values based upon the existing x and y values\n","  \n","  Required Definitions:  The function f(x,y) needs to be included in this function definition\n","  \n","  Inputs:\n","    x - a tensor with the current x values\n","    y - a tensor with the current y values (the y -values will be updated by the gradient steps)\n","    init_val - the initial value of y\n","    \n","  Outputs:\n","    The left hand side of the equation such that y' - f(x,y) = 0 and y[0] converges to the initial value\n","\n","    \n","  TBD:  Maybe consider passing in f(x,y)\n","  '''\n","  h = x[1] - x[0]  #At some point we need to compute these for each interval so we have variable spacing\n","  \n","#   def y_prime(y):\n","#     assert type(y) is torch.Tensor\n","#     return y\n","  \n","  def f(x,y):\n","    assert type(x) is torch.Tensor\n","    assert type(y) is torch.Tensor\n","    return x + 2.0 * y\n","  \n","  def k1(x,y,h):\n","    assert type(x) is torch.Tensor\n","    assert type(y) is torch.Tensor\n","    K1 = h * f(x,y)\n","    return K1\n","  \n","  def k2(x,y,h, k1):\n","    assert type(x) is torch.Tensor\n","    assert type(y) is torch.Tensor\n","    assert type(k1) is torch.Tensor\n","    K2 = h * f(x + h/2.0, y + k1/2.0)\n","    return K2\n","  \n","  def k3(x,y,h,k2):\n","    assert type(x) is torch.Tensor\n","    assert type(y) is torch.Tensor\n","    assert type(k2) is torch.Tensor\n","    K3 = h * f(x + h/2.0, y + k2/2.0)\n","    return K3\n","\n","  def k4(x,y,h, k3):\n","    assert type(x) is torch.Tensor\n","    assert type(y) is torch.Tensor\n","    assert type(k3) is torch.Tensor\n","    K4 = h * f(x+h, y + k3)\n","    return K4\n","  \n","  def F(x,y,h):\n","    assert type(x) is torch.Tensor\n","    assert type(y) is torch.Tensor\n","    \n","    K1 = k1(x,y,h)\n","    K2 = k2(x,y,h,K1)\n","    K3 = k3(x,y,h,K2)\n","    K4 = k4(x,y,h,K3)\n","    \n","    c = 1./6.\n","    Y = y + c*(K1 + 2.0 * K2 + 2.0 * K3 + K4)\n","    return Y\n","  '''\n","  Purpose:  Compute the left side of the equation to solve.  This implementaiton has the right side as zero.\n","  \n","  Inputs:\n","  x:  The x values on the interval of interest\n","  y:  The current approximation for the solution\n","  init_val:  The inital value for the left side of the interval\n","  h:  the uniform distance used in the solution\n","  \n","  Globals:\n","  global_device\n","  \n","  Output:\n","  c the result of the lhs computation of y' - F(x,y,h)\n","  '''\n","  \n","  N = y.size()[0]\n","  c = torch.zeros((N,1), device = global_device).double() #We generically use double precision\n","  c[0] = y[0] - init_val\n","  c[1:N] = y[1:N] - F(x,y,h)[0:(N-1)]\n","  return c\n","#"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3ZBGfJfXGN9D","colab_type":"code","colab":{}},"cell_type":"code","source":["diff_solve_m = diffe_solver_mm(start_x = 0.0, end_x = 2.0, num_start_intervals = 10,\n","                             lhs_function = compute_lhs, cost_function = loss_xy, init_value = .25,\n","                             epsilon = 1e-7, num_exp_average = 50)\n","\n","y = diff_solve_m.solve()\n","print(\"Target value for right end point {:.10f}\".format(.5*(np.exp(4.0)-2.0 - .5)))\n","print(\"Current approx for right end point  {:.10f}\".format(y[-1,0]))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SakTlteckCdi","colab_type":"text"},"cell_type":"markdown","source":["##  Conclusion"]},{"metadata":{"id":"xo60Zh2ukIOF","colab_type":"text"},"cell_type":"markdown","source":["This demonstrates that autograd and PyTorch have a large potential for doing investigations solving differential equations.  Using the natural expressions for the estimations makes the code easy to read and implement.  This notebook put little to no effort in optimizing the solution, but the convergence was good for all of the problems.  This approach is a practical method for early investigations that can then be converted to production applications."]}]}