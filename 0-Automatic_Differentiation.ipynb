{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"0-Automatic_Differentiation.ipynb","version":"0.3.2","provenance":[],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"_5o1S6yXrpeR","colab_type":"text"},"cell_type":"markdown","source":["# <center>Automatic Differentiation</center>\n","### <center>[Dr David Race](dr.david.race@gmail.com)</center>"]},{"metadata":{"id":"if8FECnSr8TW","colab_type":"text"},"cell_type":"markdown","source":["This notebook is specifically designed to provide a quick demonstration of \"autograd\" capabilities.  This is designed to be the first in a series on convex function minimization within Machine Learning (ML) environments, so it starts with the basics of differentiation.  This notebook uses the \"tensor\" concepts to demonstrate some of the nice methods available with both the MinPy and PyTorch packages.  These are both foundations for Deep Learning (DL) environments, but are equally adapt with some standard mathematics.\n","\n","There are two main sections:\n","1.  MinPy\n","2.  PyTorch\n","\n","The examples grow in sophistication as the notebook progresses, so be sure to follow the instructions carefully.\n","\n","<i>NOTE:  This is designed to run in Colaboratory, but is likely to run in most other Jupyter envrionments also.  In particular this does not connect to a GPU, so it will run on minimal hardware.</i>"]},{"metadata":{"id":"Uw495mjQZO9p","colab_type":"text"},"cell_type":"markdown","source":["##1.   Differentiation with MinPy.Autograd"]},{"metadata":{"id":"O1W1saFjZXIX","colab_type":"text"},"cell_type":"markdown","source":["sympy is a great package for symbolic differentiation, but sympy will just be used for simple comarison of results so you can better understand the results.  autograd is much more appropriate for larger problems so is more important for Differential Equations and Linear Algebra applications.  The reference for autograd is found at [MinPy - Autograd](https://minpy.readthedocs.io/en/latest/tutorial/autograd_tutorial.html#)."]},{"metadata":{"id":"5lFCXIzDskcw","colab_type":"text"},"cell_type":"markdown","source":["### 1.1 - Set Up Environment"]},{"metadata":{"id":"U14UhNEbs8jn","colab_type":"text"},"cell_type":"markdown","source":["This section installs Theano into the Colaboratory environment and imports the standard python numeric packages."]},{"metadata":{"id":"zt7FWfbCtZP3","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install autograd\n","#python imports\n","import os, sys\n","#numpy\n","import numpy as np\n","from numpy import linspace\n","import scipy as sp\n","#sympy\n","import sympy as smp\n","from sympy import *\n","from sympy import Function, Symbol\n","from sympy import Derivative\n","#\n","import autograd"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VLC8PVlKuNq8","colab_type":"text"},"cell_type":"markdown","source":["Thse section sets up the graphics "]},{"metadata":{"id":"vBIc2m-SuyPf","colab_type":"code","colab":{}},"cell_type":"code","source":["#The general plot capabilities\n","import matplotlib\n","import matplotlib.pyplot as plt\n","#Since these are images, turn off the grids\n","matplotlib.rc('axes',**{'grid':False})\n","#  sympy plotting\n","from sympy.plotting import plot\n","#seaborn\n","import seaborn as sns"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Tnu8s_PssQCb","colab_type":"text"},"cell_type":"markdown","source":["### 1.2 -Example 1 - $f(x) = x^2 + 4$"]},{"metadata":{"id":"039HnVTwsZj3","colab_type":"text"},"cell_type":"markdown","source":["We start with the basic problem and progress along the knowledge path."]},{"metadata":{"id":"dcVajPE5sux6","colab_type":"text"},"cell_type":"markdown","source":["#### 1.2.1  Sympy Implementation"]},{"metadata":{"id":"F5icmCJGs1OO","colab_type":"text"},"cell_type":"markdown","source":["This section uses the symbolic package to derive the known quantities for our function.  This could be done by hand, but is intended to show how the results mesh together."]},{"metadata":{"id":"QY42uvZhtJsc","colab_type":"code","colab":{}},"cell_type":"code","source":["#Define the function\n","x = Symbol('x')\n","f = Function('f')(x)\n","f = x**2 + 4\n","#Show the function definition\n","print(\"The function f\")\n","smp.pprint(f)\n","#take the derivative\n","f_prime = f.diff(x)\n","print('The derivative of f')\n","smp.pprint(f_prime)\n","#  Plot the function and derivative\n","p1 = plot(f,xlim=(-3.0,3.0),ylim=(0.0,12.0))\n","#  Compute the values of f between -3 and 3\n","f_n = lambdify(x, f, \"numpy\")\n","f_prime_n = lambdify(x,f_prime,\"numpy\")\n","x_vals = linspace(-3.0, 3.0)\n","y_vals = f_n(x_vals)\n","y_prime_vals = f_prime_n(x_vals)\n","\n","sns.set_style('dark')\n","fig, ax = plt.subplots()\n","plt.ylim(0.0,12.0)\n","plt.yticks(np.arange(1,13))\n","ax.axvline(0.0, color='k')\n","ax.axhline(0.0, color='k')\n","fn, = ax.plot(x_vals,y_vals, label='$f$')\n","fprimen, = ax.plot(x_vals,y_prime_vals, label='$\\\\frac{\\\\partial f}{\\\\partial x}$')\n","plt.legend(handles=[fn, fprimen])\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ssOJbwOH0JPD","colab_type":"text"},"cell_type":"markdown","source":["This is a standard an easily understood problem, so not much effort is put into the plot.  The main point is generation of the x and y values."]},{"metadata":{"id":"SDY9W7nM5k4U","colab_type":"text"},"cell_type":"markdown","source":["#### 1.2.2  Autograd Implementation"]},{"metadata":{"id":"nxRZT2ox5pwI","colab_type":"text"},"cell_type":"markdown","source":["Autograd understands the same type operations, but rather than a focus on symbolic computation the focus is on numeric computation using a similar underlying framework.  The main difference is that the gradient <i>(yes, these are the partial derivatives)</i> are taken relative to a scalar <i>\"loss\"</i> value.  Therefore when working with tensors of numbers, we need to define the function that will be differentiated in terms of a loss value <i>(NOTE:  The use of the loss value stems from Machine Learning.)</i>  The following code generates the same example data.\n","\n","It may not be obvious, but this provides a way to automatically compute the derivative of a function at many point concurrently.  Here is the process:\n","\n","1.  Define your function, $f$, so it inputs a tensor (vector, matrix, etc).\n","2.  Define your loss function, $loss_f$ to be $np.sum(f(x)) $\n","3.  Define the gradient of $f$ to be $grad(loss_f)$\n","4.  Then for clarity, define a function g, that outputs the $f(x)$ and $f^\\prime(x)$\n","\n","These steps are shown in the next example:\n"]},{"metadata":{"id":"c-Fsmw3k63uN","colab_type":"code","colab":{}},"cell_type":"code","source":["import autograd.numpy as np #This is so the gradient understands the numpy operations\n","from autograd import grad\n","#Follow the steps\n","\n","def f(x):\n","  y = x*x + 4.0\n","  return y\n","def loss_f(x):\n","  loss = np.sum(f(x))\n","  return loss\n","f_p = grad(loss_f)\n","def g(x):\n","  return f(x), f_p(x)\n","#Compute points\n","y, y_p = g(x_vals)\n","#plot\n","sns.set_style('dark')\n","fig, ax = plt.subplots()\n","plt.ylim(0.0,12.0)\n","plt.yticks(np.arange(1,13))\n","ax.axvline(0.0, color='k')\n","ax.axhline(0.0, color='k')\n","fn, = ax.plot(x_vals,y, label='$f$')\n","fprimen, = ax.plot(x_vals,y_p, label='$\\\\frac{\\\\partial f}{\\\\partial x}$')\n","plt.legend(handles=[fn,fprimen])\n","plt.show()\n","#\n","#  check the results\n","#\n","max_der_diff = np.max(np.abs(y_p - y_prime_vals))\n","print(\"The max difference in the derivative computation: {:.8f}\".format(max_der_diff))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Tvdwcms0DebA","colab_type":"text"},"cell_type":"markdown","source":["As you can see, ther results are exactly the same as expected and performs correctly.  Lets, see why:\n","\n","Recall from above, we defined the loss function as $np.sum(f(x))$, thus $loss_f = \\sum_{i=0}^{N-1} f(x_i)$; therefore,\n","\n","$\\frac{\\partial f(x_i)}{\\partial x_i} = f^{\\prime}(x_i)$\n","\n","since the value of $f(x_i)$ only appears once in the summation.\n","\n","Consequently, using the $np.sum$ function provides a quick way to compute the derivatives of $f$ for the input $x$ values."]},{"metadata":{"id":"zEUVPatZzZtf","colab_type":"text"},"cell_type":"markdown","source":["### Example 1.3 - $f(x,y) = x^2 + y^2 + 4$"]},{"metadata":{"id":"43X0AtraznP3","colab_type":"text"},"cell_type":"markdown","source":["In this example, we will compute the gradient of f, namely  of $grad(f) = \\nabla(f) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{bmatrix} = \\begin{bmatrix}2x \\\\ 2y\\end{bmatrix}$ for a set of random points with $x,y \\in [0,1)$.\n","\n","This works much the same as the previous example by leveraging the grad function and providing the appropriate loss function that can operate on multiple inputs concurrently."]},{"metadata":{"id":"xQ1bbC2wIrbg","colab_type":"code","colab":{}},"cell_type":"code","source":["import autograd.numpy as np #This is so the gradient understands the numpy operations\n","from autograd import grad\n","#Follow the steps\n","\n","def f(xy):\n","  z = xy[0]*xy[0] + xy[1]*xy[1] + 4.0\n","  return z\n","def loss_f(z):\n","  loss = np.sum(f(z))\n","  return loss\n","f_p = grad(loss_f)\n","def g(xy):\n","  return np.array(f(xy)),np.array(f_p(xy))\n","#Define the x and y\n","x_vals = np.random.uniform(-1,1,50)\n","y_vals = np.random.uniform(-1,1,50)\n","xy = [x_vals,y_vals]\n","#\n","#Compute points\n","z, z_p = g(xy)\n","#Compute the formula values\n","z_p_compute = np.array([2*x_vals,2*y_vals])\n","\n","max_err = np.max(np.abs(z_p - z_p_compute))\n","pprint(\"Max Error: {:8f}\".format(max_err))\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dOrv97-9RBvN","colab_type":"text"},"cell_type":"markdown","source":["Once again, the use of grad allows for easy computation of exactly the values we need for computation."]},{"metadata":{"id":"dz5GqYH9RS7s","colab_type":"text"},"cell_type":"markdown","source":["### 1.4  Conclusion"]},{"metadata":{"id":"GhZn_ep7RZcg","colab_type":"text"},"cell_type":"markdown","source":["With autograd, the computation of gradients is automatic.  Even though autograd has its primary use in Machine Learning, this tool can be very powerful for mathematics operations since it supports both GPUs and targets numpy compatibility."]},{"metadata":{"id":"vOjDdak7S9n9","colab_type":"text"},"cell_type":"markdown","source":["## 2. Differentiation with PyTorch.autograd"]},{"metadata":{"id":"r7K9NFpLTaCU","colab_type":"text"},"cell_type":"markdown","source":["The MinPy.autograd is a very nice package, but at this point PyTorch probably has a larger user community and it is also very pythonic.  Like MinPy, it has GPU support, but it doesn't overload the numpy packages.  Given its sponsors (including Facebook), the implementation for Machine Learning is very robust and it has several pre-trained models that are ready for use in solving problems.  This series of studies on using gradients generally focuses on PyTorch; however, most of the work can be done within MinPy.\n","\n","The documentation for Pytorch can be found at [Docs](https://pytorch.org/docs/stable/index.html)."]},{"metadata":{"id":"yHz7safdVRgM","colab_type":"text"},"cell_type":"markdown","source":["###2.1  Set up Environment"]},{"metadata":{"id":"BdHauRiFVeFh","colab_type":"code","colab":{}},"cell_type":"code","source":["#\n","!pip3 install -U torch\n","#\n","import torch as torch\n","import torch.tensor as T\n","import torch.autograd as t_autograd  #normally I use autograd, but I want to distinguish between MinPy autograd\n","#\n","#  Output Environment Information\n","#\n","has_cuda = torch.cuda.is_available()\n","current_device = torch.cuda.current_device() if has_cuda else -1\n","gpu_count = torch.cuda.device_count() if has_cuda else -1\n","gpu_name = torch.cuda.get_device_name(current_device) if has_cuda else \"NA\"\n","print(\"Current device {}\".format(current_device))\n","print(\"Number of devices: {:d}\".format(gpu_count))\n","print(\"Current GPU Number: {:d}\".format(current_device))\n","print(\"GPU Name: {:s}\".format(gpu_name))\n","#Set the accelerator variable\n","accelerator = 'cuda' if has_cuda else 'cpu'\n","print(\"Accelerator: {:s}\".format(accelerator))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"318PlTeAZMTm","colab_type":"text"},"cell_type":"markdown","source":["### 2.2 -Example 1 - $f(x) = x^2 + 4$"]},{"metadata":{"id":"iYUSU2sDZNyp","colab_type":"text"},"cell_type":"markdown","source":["This section solves the same problem as the previous section, but is written to accomodate a GPU so it includes some of the details to use a GPU."]},{"metadata":{"id":"Bb3iE8MWziCz","colab_type":"code","colab":{}},"cell_type":"code","source":["#define setup\n","#\n","N = 50\n","device = torch.device(accelerator)\n","#\n","#Define the function and loss\n","#\n","def f(x):\n","  y = x * x + 2.0\n","  return y\n","def loss_f(x):\n","  z = f(x).sum()\n","  return z\n","def f_p(x):\n","  z = loss_f(x)\n","  z.backward()\n","  return x.grad\n","x_val = np.linspace(-3., 3.0, N)\n","x = T(x_val, requires_grad = True).to(device)\n","#Get the data\n","x_vals = x.data.numpy()\n","y = f(x).data.numpy()\n","y_p = f_p(x).data.numpy()\n","#Graph\n","sns.set_style('dark')\n","fig, ax = plt.subplots()\n","plt.ylim(0.0,12.0)\n","plt.yticks(np.arange(1,13))\n","ax.axvline(0.0, color='k')\n","ax.axhline(0.0, color='k')\n","fn, = ax.plot(x_vals,y, label='$f$')\n","fprimen, = ax.plot(x_vals,y_p, label='$\\\\frac{\\\\partial f}{\\\\partial x}$')\n","plt.legend(handles=[fn,fprimen])\n","plt.show()\n","#\n","#  check the results\n","#\n","max_der_diff = np.max(np.abs(y_p - y_prime_vals))\n","print(\"The max difference in the derivative computation: {:.8f}\".format(max_der_diff))\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Qvtsj2dPhas-","colab_type":"text"},"cell_type":"markdown","source":["As you can see, the computations are similar to using MinPy, but instead of using a grad function this uses a backward function <i>(backward is the word in Machine Learning that computes the derivative relative to the loss)</i> and then <i>grad</i> is a property of the variable that us used for the computation that required the gradient."]},{"metadata":{"id":"aojr4FPNjiyX","colab_type":"text"},"cell_type":"markdown","source":["### Example 2.3 - $f(x,y) = x^2 + y^2 + 4$"]},{"metadata":{"id":"IX_57KHojm6c","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","#Follow the steps\n","\n","def f(xy):\n","  z = xy[0]*xy[0] + xy[1]*xy[1] + 4.0\n","  return z\n","def loss_f(z):\n","  loss = f(z).sum()\n","  return loss\n","def f_p(xy):\n","  z = loss_f(xy)\n","  z.backward()\n","  return xy.grad\n","def g(xy):\n","  return f(xy).data.numpy(),f_p(xy).data.numpy()\n","#Define the x and y\n","x_vals = np.random.uniform(-1,1,50)\n","y_vals = np.random.uniform(-1,1,50)\n","xy = T([x_vals,y_vals], requires_grad = True).to(device)\n","#\n","#Compute points\n","z, z_p = g(xy)\n","#Compute the formula values\n","z_p_compute = np.array([2*x_vals,2*y_vals])\n","\n","max_err = np.max(np.abs(z_p - z_p_compute))\n","pprint(\"Max Error: {:8f}\".format(max_err))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ze3G_wu8m9JH","colab_type":"text"},"cell_type":"markdown","source":["### 2.4 Conclusion"]},{"metadata":{"id":"pQgb7bJ1lutX","colab_type":"text"},"cell_type":"markdown","source":["PyTorch provide both a numpy compatible interface for the numpy functions, so starting with a minimal set of code using numpy, it is easy to scale up to use GPUs and PyTorch.autograd.  The interworking of PyTorch.autograd are exactly as we expect."]},{"metadata":{"id":"5lxhIy6D-GxI","colab_type":"text"},"cell_type":"markdown","source":["## Overall Conclusion"]},{"metadata":{"id":"CwnKVxGz-LQx","colab_type":"text"},"cell_type":"markdown","source":["Both MinPy and PyTorch are environments for Machine Learning, but they provide many benefits to numerical computations and modeling.  These free tools coupled with Colaboratory greatly expands the types of mathematic modeling and computations that are available to developers.  MinPy appears to have a smaller footprint, but PyTorch appears to have a larger following (especially when considering ML).  Using both isn't a bad option depending on the resources available for processing."]}]}