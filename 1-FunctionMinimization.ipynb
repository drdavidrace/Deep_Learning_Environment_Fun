{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1-FunctionMinimization.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"_5o1S6yXrpeR","colab_type":"text"},"cell_type":"markdown","source":["# <center>Function Minimization</center>\n","## <center>Starting With Empty Model</center>\n","### <center>Dr David Race</center>"]},{"metadata":{"id":"if8FECnSr8TW","colab_type":"text"},"cell_type":"markdown","source":["This notebook is specifically designed to use the Deep Learning toolkit (pytorch) to determine the minimum of a non-negative convex function.  The input will be a \"loss\" function and a starting point, then it finds the location and value of the minimum of the function."]},{"metadata":{"id":"5lFCXIzDskcw","colab_type":"text"},"cell_type":"markdown","source":["## Set Up Environment"]},{"metadata":{"id":"U14UhNEbs8jn","colab_type":"text"},"cell_type":"markdown","source":["This section installs the basic pytorch capabilities into the Colaboratory VM."]},{"metadata":{"id":"zt7FWfbCtZP3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":343},"outputId":"fd4dd6d2-7416-4b10-abfd-20aa2ca5a6ff","executionInfo":{"status":"ok","timestamp":1537971133974,"user_tz":300,"elapsed":207432,"user":{"displayName":"David Race","photoUrl":"","userId":"17739187180962963353"}}},"cell_type":"code","source":["from os import path\n","!pip3 --version\n","!pip3 install -U torch torchvision\n","accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'"],"execution_count":1,"outputs":[{"output_type":"stream","text":["pip 18.0 from /usr/local/lib/python3.6/dist-packages/pip (python 3.6)\n","Collecting torch\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/0e/e382bcf1a6ae8225f50b99cc26effa2d4cc6d66975ccf3fa9590efcbedce/torch-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (519.5MB)\n","\u001b[K    100% |████████████████████████████████| 519.5MB 25kB/s \n","tcmalloc: large alloc 1073750016 bytes == 0x58d94000 @  0x7fb4910351c4 0x46d6a4 0x5fcbcc 0x4c494d 0x54f3c4 0x553aaf 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54e4c8\n","\u001b[?25hCollecting torchvision\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n","\u001b[K    100% |████████████████████████████████| 61kB 21.5MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n","Collecting pillow>=4.1.1 (from torchvision)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/24/f53ff6b61b3d728b90934bddb4f03f8ab584a7f49299bf3bde56e2952612/Pillow-5.2.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n","\u001b[K    100% |████████████████████████████████| 2.0MB 4.3MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n","Installing collected packages: torch, pillow, torchvision\n","  Found existing installation: Pillow 4.0.0\n","    Uninstalling Pillow-4.0.0:\n","      Successfully uninstalled Pillow-4.0.0\n","Successfully installed pillow-5.2.0 torch-0.4.1 torchvision-0.2.1\n"],"name":"stdout"}]},{"metadata":{"id":"vOA2CK4Sriam","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"002521c4-c422-430a-fc28-9e4a678133ae","executionInfo":{"status":"ok","timestamp":1537971257791,"user_tz":300,"elapsed":6416,"user":{"displayName":"David Race","photoUrl":"","userId":"17739187180962963353"}}},"cell_type":"code","source":["import numpy as np\n","import torch\n","from pprint import pprint, pformat\n","pprint(accelerator)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["'cpu'\n"],"name":"stdout"}]},{"metadata":{"id":"zEUVPatZzZtf","colab_type":"text"},"cell_type":"markdown","source":["## Example 1 - $x^2 + y^2 + 4$"]},{"metadata":{"id":"43X0AtraznP3","colab_type":"text"},"cell_type":"markdown","source":["In this example, we will compute the minimum of $x^2 + y + 4^2$ starting at a random point.  The point will be initialized using randn."]},{"metadata":{"id":"Bb3iE8MWziCz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"bd993a39-933c-403c-a5a1-83a0ec79db51","executionInfo":{"status":"ok","timestamp":1537971584095,"user_tz":300,"elapsed":255,"user":{"displayName":"David Race","photoUrl":"","userId":"17739187180962963353"}}},"cell_type":"code","source":["#define setup\n","from torch.autograd import Variable\n","offset = Variable(torch.Tensor([4.]).double(), requires_grad = False)\n","mean_val = 10.\n","std_val = 20.\n","iter_stop = 1e-7\n","max_iter = 100\n","def compute_loss_error(l1, l2):\n","  return np.abs(l1 - l2)/np.max([l1,l2])\n","\n","def loss_f(in_pos):\n","  y = in_pos.pow(2).sum() + offset\n","  return y\n","  \n","device = torch.device(accelerator)\n","np.random.seed(0)\n","N = 2\n","x1 = Variable(torch.Tensor(std_val * (np.random.randn(1,N) + mean_val),device=device).double(),requires_grad = True)\n","print(\"x1\")\n","print(x1.data.numpy()[0])\n","print(\"grad of x1\")\n","print([2.0 * x1.data.numpy()[0][0], 2.0 * x1.data.numpy()[0][1]])\n","y = loss_f(x1)\n","#Optimize Using Gradient Descent\n","learning_rate = .1\n","i = 1\n","last_loss = loss_f(x1).data.numpy()[0]\n","adj_error = last_loss\n","while i<=max_iter and adj_error > iter_stop:\n","  \n","  loss = loss_f(x1)\n","  loss.backward()\n","  if(i == 1):\n","    print(\"Computed Gradient\")\n","    print(x1.grad.numpy()[0])\n","  with torch.no_grad():\n","    x1 -= learning_rate * x1.grad\n","    x1.grad.zero_()\n","  if i % 10 == 0:  \n","    curr_loss = loss_f(x1).data.numpy()[0]\n","    adj_error = compute_loss_error(last_loss,curr_loss)\n","    last_loss = curr_loss\n","  i = i + 1\n","    \n","print(\"Iteration \" + str(i))\n","print(\"Approx location\")\n","print(x1.data.numpy()[0])\n","pprint(\"Approx Value\")\n","pprint(loss_f(x1).data.numpy()[0])"],"execution_count":3,"outputs":[{"output_type":"stream","text":["x1\n","[235.28105164 208.00314331]\n","grad of x1\n","[470.5621032714844, 416.00628662109375]\n","Computed Gradient\n","[470.56210327 416.00628662]\n","Iteration 71\n","Approx location\n","[3.87156043e-05 3.42270120e-05]\n","'Approx Value'\n","4.000000002670387\n"],"name":"stdout"}]},{"metadata":{"id":"83ytPMsG3HNt","colab_type":"text"},"cell_type":"markdown","source":["You should note that the DL computation for the gradient is the same as the mathematical computation for the gradient.  This gradient is used throughout DL for training, so it is important to know that it is the same as the mathematical expectations."]},{"metadata":{"id":"8fjcaPwhSm7c","colab_type":"text"},"cell_type":"markdown","source":["## 2 - Example 1 using Stochastic Gradient Descent\n","\n","This method provides the same steps as the previous example, but the application of the gradient descent is carried out automatically by torch.optim.SGD().step().  The torch environment manages the application of the loss to the tensors to change within its framework (this is very handy for processing concepts.)"]},{"metadata":{"id":"SYqoaFJ0Tgyw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"d9ea73e3-6ccc-4291-b760-dcfe86e57450","executionInfo":{"status":"ok","timestamp":1537971594274,"user_tz":300,"elapsed":278,"user":{"displayName":"David Race","photoUrl":"","userId":"17739187180962963353"}}},"cell_type":"code","source":["#define setup\n","from torch.autograd import Variable\n","offset = Variable(torch.Tensor([4.]).double(), requires_grad = False)\n","mean_val = 10.\n","std_val = 20.\n","iter_stop = 1e-7\n","max_iter = 1000\n","def compute_loss_error(l1, l2):\n","  return np.abs(l1 - l2)/np.max([l1,l2])\n","\n","def loss_f(in_pos, target):\n","  y = in_pos.pow(2).sum() + offset\n","  return y\n","  \n","device = torch.device(accelerator)\n","N = 2\n","#reset the seed so the results are consistent\n","#\n","np.random.seed(0)\n","x1 = Variable(torch.Tensor(std_val * (np.random.randn(1,N) + mean_val),device=device).double(),requires_grad = True)\n","y = loss_f(x1,0.0)\n","#define optimizer\n","optimizer = torch.optim.SGD([x1], lr=.1)\n","#Optimize\n","i = 1\n","last_loss = loss_f(x1,0.0).data.numpy()[0]\n","adj_error = last_loss\n","while i<=max_iter and adj_error > iter_stop:\n","  optimizer.zero_grad()\n","  loss = loss_f(x1, 0.0)\n","  loss.backward()\n","  optimizer.step()\n","  if i % 10 == 0:  \n","    curr_loss = loss_f(x1,0.0).data.numpy()[0]\n","    adj_error = compute_loss_error(last_loss,curr_loss)\n","    last_loss = curr_loss\n","  i = i + 1\n","    \n","print(\"Iteration \" + str(i))\n","print(\"Approx location\")\n","print(x1.data.numpy()[0])\n","pprint(\"Approx Value\")\n","pprint(loss_f(x1,0.0).data.numpy()[0])\n","print('max relative difference')\n","pprint(np.abs(x1.data.numpy()[0][0] - x1.data.numpy()[0][1])/np.min(np.abs(x1.data.numpy()[0])))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Iteration 71\n","Approx location\n","[3.87156043e-05 3.42270120e-05]\n","'Approx Value'\n","4.000000002670387\n","max relative difference\n","0.13114180820080013\n"],"name":"stdout"}]},{"metadata":{"id":"jtP13nwqTyP5","colab_type":"text"},"cell_type":"markdown","source":["Happily, the results turn out exactly the same."]},{"metadata":{"id":"nX6sRUdFE7Ly","colab_type":"text"},"cell_type":"markdown","source":["## Example 3 - Example 1 using Adam Optimizer\n","\n","The Adam optimizer chooses a slightly different application of the step size based upon the momentum method.  This method is very useful in machine learning, but can be applied in our simple examples using larger learning rates.  This is performed in the following compute cell.  <i>(Normally there is some experimentation to find the correct learning rate, but this notebook starts using 4.0.)</i>"]},{"metadata":{"id":"Fj9nwQuqFJq-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"14acd10b-43a8-42cd-aa57-1608c52b8c28","executionInfo":{"status":"ok","timestamp":1537971608463,"user_tz":300,"elapsed":252,"user":{"displayName":"David Race","photoUrl":"","userId":"17739187180962963353"}}},"cell_type":"code","source":["#define setup\n","from torch.autograd import Variable\n","offset = Variable(torch.Tensor([4.]).double(), requires_grad = False)\n","mean_val = 10.\n","std_val = 20.\n","iter_stop = 1e-7\n","max_iter = 1000\n","def compute_loss_error(l1, l2):\n","  return np.abs(l1 - l2)/np.max([l1,l2])\n","\n","def loss_f(in_pos, target):\n","  y = in_pos.pow(2).sum() + offset\n","  return y\n","  \n","device = torch.device(accelerator)\n","N = 2\n","np.random.seed(0)\n","x1 = Variable(torch.Tensor(std_val * (np.random.randn(1,N) + mean_val),device=device).double(),requires_grad = True)\n","y = loss_f(x1,0.0)\n","#define optimizer\n","optimizer = torch.optim.Adam([x1], lr=4., betas=(.5,.9))\n","#Optimize\n","i = 1\n","last_loss = loss_f(x1,0.0).data.numpy()[0]\n","adj_error = last_loss\n","while i<=max_iter and adj_error > iter_stop:\n","  optimizer.zero_grad()\n","  loss = loss_f(x1, 0.0)\n","  loss.backward()\n","  optimizer.step()\n","  if i % 10 == 0:  \n","    curr_loss = loss_f(x1,0.0).data.numpy()[0]\n","    adj_error = compute_loss_error(last_loss,curr_loss)\n","    last_loss = curr_loss\n","  i = i + 1\n","    \n","print(\"Iteration \" + str(i))\n","print(\"Approx location\")\n","print(x1.data.numpy()[0])\n","pprint(\"Approx Value\")\n","pprint(loss_f(x1,0.0).data.numpy()[0])\n","print('max relative difference')\n","pprint(np.abs(x1.data.numpy()[0][0] - x1.data.numpy()[0][1])/np.min(np.abs(x1.data.numpy()[0])))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Iteration 121\n","Approx location\n","[-2.7381728e-07  5.1987704e-07]\n","'Approx Value'\n","4.0000000000003455\n","max relative difference\n","2.898627582864771\n"],"name":"stdout"}]},{"metadata":{"id":"Tb8kuD7FVaCw","colab_type":"text"},"cell_type":"markdown","source":["The Adam optimizer is much better when there are large numbers of variables (such as with Deep Learning), but this example shows that the tools within the \"torch\" toolkit work as we expect them to in computational environments."]},{"metadata":{"id":"X4dkLF_9Vw7V","colab_type":"text"},"cell_type":"markdown","source":["## Example 4 - Similar to Example 1, but using $w^6 + x^6 + y^6 + z^6 + 4$"]},{"metadata":{"id":"K6EOP-ezXJOl","colab_type":"text"},"cell_type":"markdown","source":["To demonstrate that the problem complexity matters for the gradient method of choice we use the function $w^6 + x^6 + y^6 + z^6 + 4$.  In the following few cells we use the SGD and Adam to study the general capabilities."]},{"metadata":{"id":"J7mv_Q-O2Bhn","colab_type":"text"},"cell_type":"markdown","source":["#### Gradient Descent"]},{"metadata":{"id":"a235xuqyWGXI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":153},"outputId":"18a37d34-8259-4841-8c5d-77c13d4d8b5e","executionInfo":{"status":"ok","timestamp":1537971623881,"user_tz":300,"elapsed":255,"user":{"displayName":"David Race","photoUrl":"","userId":"17739187180962963353"}}},"cell_type":"code","source":["#define setup\n","from torch.autograd import Variable\n","offset = Variable(torch.Tensor([4.]).double(), requires_grad = False)\n","mean_val = 2.0\n","std_val = 2.0\n","iter_stop = 1e-7\n","max_iter = 10000\n","def compute_loss_error(l1, l2):\n","  return np.abs(l1 - l2)/np.max([l1,l2])\n","\n","def loss_f(in_pos):\n","  y = in_pos.pow(6).sum() + offset\n","  return y\n","  \n","device = torch.device(accelerator)\n","np.random.seed(0)\n","N = 4\n","x1 = Variable(torch.Tensor(std_val * (np.random.randn(1,N) + mean_val),device=device).double(),requires_grad = True)\n","y = loss_f(x1)\n","\n","pprint(y.data.numpy())\n","#Optimize Using Gradient Descent\n","learning_rate = .001\n","i = 1\n","last_loss = loss_f(x1).data.numpy()[0]\n","adj_error = last_loss\n","while i<=max_iter and adj_error > iter_stop:\n","  \n","  loss = loss_f(x1)\n","  loss.backward()\n","  with torch.no_grad():\n","    x1 -= learning_rate * x1.grad\n","    x1.grad.zero_()\n","  if i % 10 == 0:  \n","    curr_loss = loss_f(x1).data.numpy()[0]\n","    adj_error = compute_loss_error(last_loss,curr_loss)\n","    last_loss = curr_loss\n","  i = i + 1\n","    \n","print(\"Iteration \" + str(i))\n","print(\"Approx location\")\n","print(x1.data.numpy()[0])\n","pprint(\"Approx Value\")\n","pprint(loss_f(x1).data.numpy()[0])"],"execution_count":6,"outputs":[{"output_type":"stream","text":["array([611290.67773046])\n","Iteration 11\n","Approx location\n","[nan nan nan nan]\n","'Approx Value'\n","nan\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:26: RuntimeWarning: invalid value encountered in reduce\n","  return umr_maximum(a, axis, None, out, keepdims)\n"],"name":"stderr"}]},{"metadata":{"id":"zdFvE0UpzvuH","colab_type":"text"},"cell_type":"markdown","source":["Notice that this blew up with the SGD optimizer, but this is fairly typical of many problems.  SGD works well when the starting point is close to the target and when the problem is quadratic, but doesn't work particularily well in many other cases.  The user can run experiments with the learning_rate to improve performance, but the improvements do not happen very fast."]},{"metadata":{"id":"aE9dl-Po53ng","colab_type":"text"},"cell_type":"markdown","source":["#### Adam"]},{"metadata":{"id":"X6YLrUkTXdk9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"46b79f26-3782-439d-8b0a-2ec12e1ca88e","executionInfo":{"status":"ok","timestamp":1537971661482,"user_tz":300,"elapsed":376,"user":{"displayName":"David Race","photoUrl":"","userId":"17739187180962963353"}}},"cell_type":"code","source":["#define setup\n","from torch.autograd import Variable\n","offset = Variable(torch.Tensor([4.]).double(), requires_grad = False)\n","mean_val = 2.\n","std_val = 2.\n","iter_stop = 1e-7\n","max_iter = 10000\n","def compute_loss_error(l1, l2):\n","  return np.abs(l1 - l2)/np.max([l1,l2])\n","\n","def loss_f(in_pos, target):\n","  y = in_pos.pow(6).sum() + offset\n","  return y\n","  \n","device = torch.device(accelerator)\n","N = 4\n","np.random.seed(0)\n","x1 = Variable(torch.Tensor(std_val * (np.random.randn(1,N) + mean_val),device=device).double(),requires_grad = True)\n","y = loss_f(x1,0.0)\n","#define optimizer\n","optimizer = torch.optim.Adam([x1], lr=.04, betas=(.5,.9))\n","#Optimize\n","i = 1\n","last_loss = loss_f(x1,0.0).data.numpy()[0]\n","adj_error = last_loss\n","while i<=max_iter and adj_error > iter_stop:\n","  optimizer.zero_grad()\n","  loss = loss_f(x1, 0.0)\n","  loss.backward()\n","  optimizer.step()\n","  if i % 10 == 0:  \n","    curr_loss = loss_f(x1,0.0).data.numpy()[0]\n","    adj_error = compute_loss_error(last_loss,curr_loss)\n","    last_loss = curr_loss\n","  i = i + 1\n","    \n","print(\"Iteration \" + str(i))\n","print(\"Approx location\")\n","print(x1.data.numpy()[0])\n","pprint(\"Approx Value\")\n","pprint(loss_f(x1,0.0).data.numpy()[0])\n","print('max relative difference')\n","pprint(np.abs(x1.data.numpy()[0][0] - x1.data.numpy()[0][1])/np.min(np.abs(x1.data.numpy()[0])))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Iteration 491\n","Approx location\n","[0.05359965 0.01601169 0.02730643 0.079249  ]\n","'Approx Value'\n","4.000000271864511\n","max relative difference\n","2.347532030008755\n"],"name":"stdout"}]},{"metadata":{"id":"yXUyBFO43ppT","colab_type":"text"},"cell_type":"markdown","source":["The Adam optimizer is more robust for finding solutions rather than exploding or developing vanishing gradients.  There are always other optimizers to learn, but the use of the momentum factors improves the robutness of the convergence.  One item that is beginning to get notice is the type of solution with Adam, namely, in this simple case the maximum relative difference is much higher than with the SGD.\n","\n","There is some thought that this might lead to less \"generalized\" solutions in some problems."]},{"metadata":{"id":"xKDg96CE59Fv","colab_type":"text"},"cell_type":"markdown","source":["#### Adam with Multiple Optimizers"]},{"metadata":{"id":"uJp0ZMcl6Eon","colab_type":"text"},"cell_type":"markdown","source":["On complex problems, it has been observed that changing the betas as you get closer to a solution often leads to faster convergence.  In the next two cells, notice that even on our simple problems and using very simple criteria to change the optimizers we do achieve faster convergence.  pytorch is particularily easy to use in this regard since the compute graphs are dynamically computed.  <b>(This is a potentially strong feature for pytorch.)</b>"]},{"metadata":{"id":"T_Zqs6OxxIwB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"9b533e85-bc5c-4fb1-e060-ce35d42faaba","executionInfo":{"status":"ok","timestamp":1537971704139,"user_tz":300,"elapsed":438,"user":{"displayName":"David Race","photoUrl":"","userId":"17739187180962963353"}}},"cell_type":"code","source":["#define setup\n","from torch.autograd import Variable\n","offset = Variable(torch.Tensor([4.]).double(), requires_grad = False)\n","mean_val = 2.\n","std_val = 2.\n","iter_stop = 1e-7\n","max_iter = 10000\n","def compute_loss_error(l1, l2):\n","  return np.abs(l1 - l2)/np.max([l1,l2])\n","\n","def loss_f(in_pos, target):\n","  y = in_pos.pow(6).sum() + offset\n","  return y\n","  \n","device = torch.device(accelerator)\n","N = 4\n","np.random.seed(0)\n","x1 = Variable(torch.Tensor(std_val * (np.random.randn(1,N) + mean_val),device=device).double(),requires_grad = True)\n","y = loss_f(x1,0.0)\n","#define optimizer\n","optimizer = torch.optim.Adam([x1], lr=.04, betas=(.5,.9))\n","optimizer2 = torch.optim.Adam([x1], lr=.04, betas=(.9,.95))\n","optimizer3 = torch.optim.Adam([x1], lr=.04, betas=(.9,.999))\n","#Optimize\n","i = 1\n","last_loss = loss_f(x1,0.0).data.numpy()[0]\n","adj_error = last_loss\n","while i<=max_iter and adj_error > iter_stop:\n","  if adj_error > .1:\n","    optimizer.zero_grad()\n","    loss = loss_f(x1, 0.0)\n","    loss.backward()\n","    optimizer.step()\n","  else:\n","    optimizer2.zero_grad()\n","    loss = loss_f(x1, 0.0)\n","    loss.backward()\n","    optimizer2.step()   \n","  if i % 10 == 0:  \n","    curr_loss = loss_f(x1,0.0).data.numpy()[0]\n","    adj_error = compute_loss_error(last_loss,curr_loss)\n","    last_loss = curr_loss\n","  i = i + 1\n","    \n","print(\"Iteration \" + str(i))\n","print(\"Approx location\")\n","print(x1.data.numpy()[0])\n","pprint(\"Approx Value\")\n","pprint(loss_f(x1,0.0).data.numpy()[0])\n","print('max relative difference')\n","pprint(np.abs(x1.data.numpy()[0][0] - x1.data.numpy()[0][1])/np.min(np.abs(x1.data.numpy()[0])))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Iteration 381\n","Approx location\n","[-0.02688861  0.0072186  -0.0576078   0.09676172]\n","'Approx Value'\n","4.0000008576979225\n","max relative difference\n","4.724905117913807\n"],"name":"stdout"}]},{"metadata":{"id":"dL4bXonzv0uk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"7fc66b34-b3b5-49f5-bef8-d9764689f799","executionInfo":{"status":"ok","timestamp":1537971713274,"user_tz":300,"elapsed":347,"user":{"displayName":"David Race","photoUrl":"","userId":"17739187180962963353"}}},"cell_type":"code","source":["#define setup\n","from torch.autograd import Variable\n","offset = Variable(torch.Tensor([4.]).double(), requires_grad = False)\n","mean_val = 2.\n","std_val = 2.\n","iter_stop = 1e-7\n","max_iter = 10000\n","def compute_loss_error(l1, l2):\n","  return np.abs(l1 - l2)/np.max([l1,l2])\n","\n","def loss_f(in_pos, target):\n","  y = in_pos.pow(6).sum() + offset\n","  return y\n","  \n","device = torch.device(accelerator)\n","N = 4\n","np.random.seed(0)\n","x1 = Variable(torch.Tensor(std_val * (np.random.randn(1,N) + mean_val),device=device).double(),requires_grad = True)\n","y = loss_f(x1,0.0)\n","#define optimizer\n","optimizer = torch.optim.Adam([x1], lr=.04, betas=(.5,.9))\n","optimizer2 = torch.optim.Adam([x1], lr=.04, betas=(.9,.95))\n","optimizer3 = torch.optim.Adam([x1], lr=.04, betas=(.9,.999))\n","#Optimize\n","i = 1\n","last_loss = loss_f(x1,0.0).data.numpy()[0]\n","adj_error = last_loss\n","while i<=max_iter and adj_error > iter_stop:\n","  if adj_error > .1:\n","    optimizer.zero_grad()\n","    loss = loss_f(x1, 0.0)\n","    loss.backward()\n","    optimizer.step()\n","  elif adj_error > .01:\n","    optimizer2.zero_grad()\n","    loss = loss_f(x1, 0.0)\n","    loss.backward()\n","    optimizer2.step()\n","  else:\n","    optimizer3.zero_grad()\n","    loss = loss_f(x1, 0.0)\n","    loss.backward()\n","    optimizer3.step()    \n","  if i % 10 == 0:  \n","    curr_loss = loss_f(x1,0.0).data.numpy()[0]\n","    adj_error = compute_loss_error(last_loss,curr_loss)\n","    last_loss = curr_loss\n","  i = i + 1\n","    \n","print(\"Iteration \" + str(i))\n","print(\"Approx location\")\n","print(x1.data.numpy()[0])\n","pprint(\"Approx Value\")\n","pprint(loss_f(x1,0.0).data.numpy()[0])\n","print('max relative difference')\n","pprint(np.abs(x1.data.numpy()[0][0] - x1.data.numpy()[0][1])/np.min(np.abs(x1.data.numpy()[0])))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Iteration 351\n","Approx location\n","[-0.00938893  0.01379486 -0.02822738 -0.05194254]\n","'Approx Value'\n","4.000000020153327\n","max relative difference\n","2.4692690893153793\n"],"name":"stdout"}]},{"metadata":{"id":"25pUChA-6bzw","colab_type":"text"},"cell_type":"markdown","source":["On this simple problem, the choice of different optimizers at different steps yielded a 28.5% performance improvement.  This is certainly something to consider when training long running NN.  It should be noted that the maximum relative difference in these tests are still fairly high compared to the initlal tests with SGD."]},{"metadata":{"id":"euPaeLCn8CAY","colab_type":"text"},"cell_type":"markdown","source":["####  Using Multiple Optimizers with a Finish Optimizer"]},{"metadata":{"id":"9VrrN44z8fCy","colab_type":"text"},"cell_type":"markdown","source":["As discussed in [Improving Generalization Performance by Switching from Adam to SGD](https://arxiv.org/abs/1712.07628), it might be possible to improve stability by using Adam (or variants early) then switching to SGD (or variants later).  The next cell demonstrates this possibility for our simple problem.  The convergence isn't as fast as using Adam, but the resulting solution is smoother as seen in the original problem.  This is certainly worth considering."]},{"metadata":{"id":"unu9AMwl0r9N","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"6747c3ca-e59c-4fef-ad20-2e086f4d711c","executionInfo":{"status":"ok","timestamp":1537971731093,"user_tz":300,"elapsed":507,"user":{"displayName":"David Race","photoUrl":"","userId":"17739187180962963353"}}},"cell_type":"code","source":["#define setup\n","from torch.autograd import Variable\n","offset = Variable(torch.Tensor([4.]).double(), requires_grad = False)\n","mean_val = 2.\n","std_val = 2.\n","iter_stop = 1e-7\n","max_iter = 10000\n","def compute_loss_error(l1, l2):\n","  return np.abs(l1 - l2)/np.max([l1,l2])\n","\n","def loss_f(in_pos, target):\n","  y = in_pos.pow(6).sum() + offset\n","  return y\n","  \n","device = torch.device(accelerator)\n","N = 4\n","np.random.seed(0)\n","x1 = Variable(torch.Tensor(std_val * (np.random.randn(1,N) + mean_val),device=device).double(),requires_grad = True)\n","y = loss_f(x1,0.0)\n","#define optimizer\n","optimizer = torch.optim.Adam([x1], lr=.04, betas=(.5,.9))\n","optimizer2 = torch.optim.SGD([x1], lr=.1)\n","#Optimize\n","i = 1\n","last_loss = loss_f(x1,0.0).data.numpy()[0]\n","adj_error = last_loss\n","while i<=max_iter and adj_error > iter_stop:\n","  if adj_error > .1:\n","    optimizer.zero_grad()\n","    loss = loss_f(x1, 0.0)\n","    loss.backward()\n","    optimizer.step()\n","  else:\n","    optimizer2.zero_grad()\n","    loss = loss_f(x1, 0.0)\n","    loss.backward()\n","    optimizer2.step()   \n","  if i % 10 == 0:  \n","    curr_loss = loss_f(x1,0.0).data.numpy()[0]\n","    adj_error = compute_loss_error(last_loss,curr_loss)\n","    last_loss = curr_loss\n","  i = i + 1\n","    \n","print(\"Iteration \" + str(i))\n","print(\"Approx location\")\n","print(x1.data.numpy()[0])\n","pprint(\"Approx Value\")\n","pprint(loss_f(x1,0.0).data.numpy()[0])\n","print('max relative difference')\n","pprint(np.abs(x1.data.numpy()[0][0] - x1.data.numpy()[0][1])/np.min(np.abs(x1.data.numpy()[0])))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Iteration 1331\n","Approx location\n","[0.14189894 0.12888619 0.14013529 0.14196186]\n","'Approx Value'\n","4.000028505918215\n","max relative difference\n","0.10096309252743065\n"],"name":"stdout"}]},{"metadata":{"id":"5lxhIy6D-GxI","colab_type":"text"},"cell_type":"markdown","source":["## Conclusion"]},{"metadata":{"id":"CwnKVxGz-LQx","colab_type":"text"},"cell_type":"markdown","source":["It is comforting to see that the optimization methods in DL are variations on mathematical convex optimizations; futhermore, the entire optimization environment has a lot of flexibility for solving problems.  This is probably as good as any environment for working with larger and more complex problems since you don't have to do much of the gradient computation manually."]}]}