{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Gradient_Descent_Neural_Style_Transfer.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"nq_B010XthHd","colab_type":"text"},"cell_type":"markdown","source":["\n","#<center> <b>Neural Transfer Using PyTorch</b></center>\n","## <center> Studying Gradient Descent Methods</center>\n","###<center>dr.david.race@gmail.com</center>\n","\n","\n","This is a quick extension of the PyTorch tutorial by Alexis Jacq [NST Tutorial](https://pytorch.org/tutorials/advanced/neural_style_tutorial.html), but designed to run within the Colaboratory environment with a Google Drive as the permanent storage area.  The particular choice for using Google Drive is driven by my personal transition to a Chromebox/book environment; therefore, optimization of this change requires minimizing any transfers to/from my local drive.\n","\n","##Introduction\n","There is a lot of activity in this notebook since we need to:\n","1.  Demostrate moving data to/from a Google Drive\n","2.  Demonstrate installing and using PyTorch\n","3.  Using gradient methods in PyTorch while leveraging and extending a previously trained model.\n","\n","My main focus is on the gradient methods, but the other two pieces are also informative for solving future problems.\n","There are a couple of challenges, namely:\n","\n","This notebook doesn't spend a lot of time on the Neural Style Transfer algorithm (since it is readily available at [Tutorial ](https://pytorch.org/tutorials/advanced/neural_style_tutorial.html#sphx-glr-download-advanced-neural-style-tutorial-py)), rather, the focus is on leveraging gradient descent and Colaboratory to do something interesting.  This notebook has four main paragraphs:\n","\n","1.  Setting up the  Environment\n","2.  Performing the Initial Image Processing\n","3.  Using Gradient Descent for Neural Transfer\n","4.  Saving the Result\n","\n","NOTE:  This can be done using just a cpu; however, the GPU processing is much faster.  Consequently using a GPU is recommended for this notebook.  Since this notebook is assumed to be running in Colaboratory, you should change the runtime to include a GPU."]},{"metadata":{"id":"aWEi3nSy56Kb","colab_type":"text"},"cell_type":"markdown","source":["##1.  Set up the Environment\n","\n","After we load the torch capabilities, we need to restart to runtime so that the PIL capabilities are synched between the default on the Colaboratory environment (the old verion 4.x) and the requirement for torch (the newer verion 5.x).  Otherwise everything is fairly standard, so the main steps are:\n","\n","<ol type=\"a\">\n","  <li>Import my standard python environment: os, sys, numpy, scipy, matplotlib, skimage tools</li>\n","  <li>Install/import torch and torchvision</li>\n","  <li>Update the Python Environment</li>\n","  <li>Copy the two working files from my Google Drive</li>\n","</ol>\n","\n"]},{"metadata":{"id":"X3KWNihrsy7X","colab_type":"text"},"cell_type":"markdown","source":["###1.a  Import Standard Python"]},{"metadata":{"id":"uknRpA3o9yu9","colab_type":"text"},"cell_type":"markdown","source":["python is a rich environment, but these import the standard imports in the first compute cell and what I consider the specific ones for this notebook in the second cell."]},{"metadata":{"id":"xCQpN8kotKWJ","colab_type":"code","colab":{}},"cell_type":"code","source":["#General python includes\n","#skimage is use for much of the preprocessing\n","import os, sys\n","import os.path\n","import numpy as np\n","import scipy as sp\n","from pprint import pprint, pformat\n","from itertools import product\n","import skimage\n","from skimage import io, transform\n","#\n","#The plot capabilities\n","import matplotlib\n","import matplotlib.image as mpimg\n","import matplotlib.pyplot as plt\n","#Since these are images, turn off the grids\n","matplotlib.rc('axes',**{'grid':False})\n","#Output system specific informaiton\n","print(\"Current Working Directory: {:s}\".format(os.getcwd()))\n","_content_path_ = os.getcwd()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AnHg1ik9t0Y1","colab_type":"code","colab":{}},"cell_type":"code","source":["#To perform deep copies\n","import copy"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uQaVXgbfuQl4","colab_type":"text"},"cell_type":"markdown","source":["###1.b  Install and Import Torch/Torchvision\n","\n","The Colaboratory environment is very nice because you have the option of using a GPU.  Once a data scientist moves beyond the simple examples, the choice of using a GPU really isn't a choice; therefore, a user must choose an environment.  The obvious choice is Tensorflow <i>(since it is embedded in Colaboratory)</i>, but personal preferences come in to play choosing between Tensorflow and PyTorch.  I have based my own choice of Torch <i>(I don't use it for my building class labs because it requires a few extra steps.)</i>  on the following observations -\n","<ol type=\"i\">\n","  <li>PyTorch feels more pythonic</li>\n","  <li>PyTorch recomputes the gradient dynamically; therefore, we can easily mix/match gradient techniques within a module</li>\n","  <li>The entendability of pretrained Deep Learning models seems more natural</li>\n","</ol>\n","These are just personal preferences, but they work in this notebook."]},{"metadata":{"id":"MeFjQNPy6Xxk","colab_type":"code","colab":{}},"cell_type":"code","source":["#\n","#Install Torch and TorchVision, torchvision is the home to vgg models\n","!pip3 install -U torch torchvision\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import transforms\n","from torchvision import models\n","#  import PIL since torchvision uses PIL\n","import PIL\n","from PIL import Image\n","#\n","#  Output Information\n","#\n","has_cuda = torch.cuda.is_available()\n","current_device = torch.cuda.current_device() if has_cuda else -1\n","gpu_count = torch.cuda.device_count() if has_cuda else -1\n","gpu_name = torch.cuda.get_device_name(current_device) if has_cuda else \"NA\"\n","print(\"Number of devices: {:d}\".format(gpu_count))\n","print(\"Current GPU Number: {:d}\".format(current_device))\n","print(\"GPU Name: {:s}\".format(gpu_name))\n","#Set the accelerator variable\n","accelerator = 'cuda' if has_cuda else 'cpu'\n","print(\"Accelerator: {:s}\".format(accelerator))\n","\n","#"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2JjjSL-uzKCJ","colab_type":"text"},"cell_type":"markdown","source":["###Update the Python Environment"]},{"metadata":{"id":"5iUXguzD-gBD","colab_type":"text"},"cell_type":"markdown","source":["At this point, I apologize for the inconvenience, but restart the current runtime.  After restart, re-execute the previous compute cells and skip this action.  <i>(If anyone knows an easier way to accomplish these updates, please drop me a note.)</i>"]},{"metadata":{"id":"iY97_nc--aVy","colab_type":"text"},"cell_type":"markdown","source":["###1.d  Copy the Two Working Files from Google Drive\n","\n","Per my transition to using the web for data storage, this notebook assumes all permanent storage is on a Google Drive.  Anyone using Colaboratory has a Google Drive; therefore, this is a nice optimization.  To optimize this approach, a personal library was developed <i>(and available at [Connection To Google Drive](https://github.com/drdavidrace/colab_gdrive))</i> that is more \"Linux\" like to avoid obtaining the file_id for each individual file of interest.  The library uses the \"file path\" to identify a file and contains methods for copying the file to the local directory.  For this we need to :\n","\n","*   Connect to the Google Drive\n","*   Copy the files to the local VM\n","\n","We will use the connection to save the file at a later point.\n","\n","Connect to your Google Drive -"]},{"metadata":{"id":"M-fKEDvL-xCz","colab_type":"code","colab":{}},"cell_type":"code","source":["#Connect to your Google Drive\n","import logging\n","#\n","!pip uninstall --yes colab_gdrive\n","!pip install -U -q git+https://github.com/drdavidrace/colab_gdrive.git\n","!pip list | grep -i colab\n","!pip install -U -q PyDrive\n","#\n","#  Connect to a ColabGDrive\n","from colab_gdrive import colab_gdrive\n","\n","myGdrive = colab_gdrive.ColabGDrive(logging_level=logging.ERROR)\n","pprint(myGdrive.is_connected())\n","pprint(myGdrive.getcwd())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8mZQcTsp_khf","colab_type":"text"},"cell_type":"markdown","source":["Download your images -\n","\n","This uses a form, which must be run before proceeding.  "]},{"metadata":{"id":"u2Q3-i1E5uK5","colab_type":"code","colab":{}},"cell_type":"code","source":["#@title Run this cell to update the files and default locations\n","content_photo = 'Self.jpg' #@param {type:\"string\"}\n","style_photo = 'picasso.jpg' #@param {type:\"string\"}\n","google_drive_path = 'BigDataTraining/FunWithGradientDescent' #@param{type:\"string\"}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JEiHz1XH_nKv","colab_type":"code","colab":{}},"cell_type":"code","source":["#Define files to copy \n","#change the next three lines to match your files\n","content_photo = content_photo.strip()\n","style_photo = style_photo.strip()\n","google_drive_path = google_drive_path.strip()\n","#\n","#  Change the google drive path\n","#\n","myGdrive.chdir(google_drive_path)\n","#\n","data_files = [content_photo, style_photo]\n","#Check local file existence\n","files_to_copy = []\n","for df in data_files:\n","  if not os.path.isfile(df):\n","    image_name = df\n","    files_to_copy.append(image_name)\n","#copy files\n","pprint(files_to_copy)\n","if files_to_copy:\n","  myGdrive.copy_from(files_to_copy)\n","#Check local file existence\n","for df in data_files:\n","  if os.path.isfile(df):\n","    pprint(\"Found local version of \" + df)\n","  else:\n","    pprint(\"Did not find local version of \" + df)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fxtabQiP_1xz","colab_type":"text"},"cell_type":"markdown","source":["These files should be stored in the \"/content\" directory, so run a visual check for the files."]},{"metadata":{"id":"8u4BxN0U_42r","colab_type":"code","colab":{}},"cell_type":"code","source":["!pwd\n","!ls -alF"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iHMGD2dt7iB6","colab_type":"text"},"cell_type":"markdown","source":["###  Conclusion"]},{"metadata":{"id":"XfHuNdPk7lY_","colab_type":"text"},"cell_type":"markdown","source":["At this point the environment is set up and the data is ready for processing."]},{"metadata":{"id":"l4vIPBTgAOST","colab_type":"text"},"cell_type":"markdown","source":["##2.  Perform the Initial Image Processing"]},{"metadata":{"id":"6RV8PWYIAaAL","colab_type":"text"},"cell_type":"markdown","source":["I have always liked the Picasso effects, but style tranfer alone isn't sufficient to achieve an interesting look.  To achieve the desired effects, the initial image processing performs these other steps:\n","\n","*  Permuting the squares of the images (I experimented with different size squares, but 3x3 decompositon seems to work well for my photo.)\n","*  Rotating the squares of the images (I don't rotate many of the squares, but this gives a nice look.)\n","*  Doing a blue color shift (this can be adjusted.)\n","\n","Since I am using a 224x224 image (primarily for my personal photo), I first sample to 225x225 for the rotations.  Then later resample to the target size of 224x224.\n","\n","For the permutation of the square, this supports both a random permutation or a fixed permutation.  Random was nice to start, but I eventually went with fixed since I am only working with a single image.\n","\n","For the rotations, this codel only supports rotations of 0, $\\frac{\\pi}{2}$,  $\\pi$, and $\\frac{\\pi}{2}$ degrees (so I don't have to do some type of mapping from the other rotated squares to the target image squares).  \n","\n","For the color shift, this is rather rudimentary.  This just adds a shift of <i>blue_shift</i> to the blue and subtracts $\\frac{blue_shift}{2}$ from the other two colors. \n","\n","><i>Note:  After working with this a little, I have chosen to not do the rotation.  I have left the code so other can have fun with it as desired.</i>\n","\n","\n","\n","Set the processing variables:  (You must run the form cell before the values will be set.)"]},{"metadata":{"id":"uia68stpAdXX","colab_type":"code","colab":{},"cellView":"form"},"cell_type":"code","source":["#@title  After running this cell the first time, it changes automatically on a change.\n","do_permute = True #@param [\"False\", \"True\"]{type:\"raw\"}\n","do_rotation = True #@param [\"False\", \"True\"]{type:\"raw\"}\n","do_shift = True #@param [\"False\", \"True\"]{type:\"raw\"}\n","random_permute = False #@param [\"False\", \"True\"] {type:\"raw\"}\n","blue_shift = .1 #@param {type:\"number\"}\n","target_image_rows = 224 #@param {type:\"integer\"}\n","target_image_cols = 224 #@param {type: \"integer\"}\n","number_blocks = 3 #@param {type: \"integer\"}\n","content_image_file = \"Self.jpg\" #@param{type: \"string\"}\n","style_image_file = \"picasso.jpg\" #@param{type: \"string\"}\n","stage_one_output_file = \"working_image_one.jpg\" #@param{type: \"string\"}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Xe1ls3hUA0xW","colab_type":"text"},"cell_type":"markdown","source":["Define the basic functions."]},{"metadata":{"id":"Zn3jdENzA29e","colab_type":"code","colab":{}},"cell_type":"code","source":["from math import sqrt\n","#\n","#  functions\n","#\n","def make_content_name(in_name):\n","  '''\n","  Purpose:  Create a complete file name based upon the base directory: /content\n","  \n","  Input:\n","    in_name - The name of the input file\n","    \n","  Uses:\n","    _content_path_ - The global variable that stores the current content path\n","    \n","  Output:\n","    The concatenation of the content_path with the in_name\n","  '''\n","  return os.path.join(_content_path_,in_name)\n","\n","def print_move_matrix(in_matrix):\n","  '''\n","  Purpose:  Only used during debugging, but left here.\n","  '''\n","  m,nx,ny = in_matrix.shape\n","  for i,j in product(range(nx),range(ny)):\n","    pprint('{:d},{:d} -> {:d},{:d}]'.format(i,j,in_matrix[0,i,j],in_matrix[1,i,j]))\n","def compute_start_rc(k, num_sq, sq_size):\n","  '''\n","  Purpose:  compute the start row and start pixel of a particular square used in the permuatation of the squares\n","  \n","  Inputs:\n","    k:  number of the chosen square (row major order)\n","    num_sq:  The number of squares in each row and column\n","    sq_size:  The number of pixels in each square (sq_size x sq_size)\n","    \n","  Outputs:\n","    row_start:  The row the data to start cut out\n","    col_start:  The column of the data to start cut out\n","  '''\n","  assert k >= 0\n","  assert num_sq > 0\n","  assert sq_size > 0\n","  \n","  row = int(k / num_sq)\n","  col = k - row * num_sq\n","  row_start = row * sq_size\n","  col_start = col * sq_size\n","  return row_start, col_start\n","\n","def copy_image_part(in_image, k, num_sq):\n","  '''\n","  Purpose:  Create a copy of an square cut out of an image\n","  \n","  Inputs:\n","    in_image:  The original image\n","    k:  The square number to cut out\n","    num_sq:  The number of square to use for image cut outs\n","    \n","  Outputs:\n","    A copy of the data in the square to cut out\n","  '''\n","  nx,ny,m = in_image.shape\n","  sq_size = int(nx/num_sq)\n","  assert sq_size * num_sq == nx\n","  assert nx == ny\n","  row_start, col_start = compute_start_rc(k, num_sq, sq_size)\n","  temp_array = in_image[row_start:row_start + sq_size, col_start:col_start + sq_size,:].copy()\n","  return temp_array\n","\n","def permute_image(in_image, permute_array, num_sq):\n","  '''\n","  Purpose:  Permute the square in the image\n","  \n","  Inputs:\n","    in_image: The original image\n","    permute_array:  An array defining the permutation\n","    num_sq:  The number of squares in each row and column\n","    \n","  Outputs:\n","    A copy of the image with the squares permuted\n","  '''\n","  nx,ny,m = in_image.shape\n","  w_image = in_image.copy()\n","  assert nx == ny\n","  sq_size = int(nx/num_sq)\n","  assert sq_size  * num_sq == nx\n","  work_array = np.zeros((sq_size,sq_size,m))\n","  temp_array = np.zeros((sq_size,sq_size,m))\n","  touched = np.full((len(permute_array)),False,dtype=bool)\n","  cur_no = 0\n","  work_array = w_image[:sq_size,:sq_size,:].copy()\n","  next_no = 0\n","  while not np.all(touched):\n","\n","    next_no = permute_array[cur_no]\n","    if not touched[next_no]:\n","      temp_array = copy_image_part(w_image,next_no,num_sq)\n","      row_start, col_start = compute_start_rc(next_no, num_sq, sq_size)\n","      w_image[row_start:row_start + sq_size, col_start:col_start + sq_size,:] = work_array.copy()\n","      work_array = temp_array.copy()\n","    else:\n","      for i in range(len(permute_array)):\n","        if not touched[i]:\n","          cur_no = i\n","          break\n","      work_array = copy_image_part(w_image, cur_no, num_sq)\n","      next_no = permute_array[cur_no]\n","      temp_array = copy_image_part(w_image,next_no,num_sq)\n","      row_start, col_start = compute_start_rc(next_no, num_sq, sq_size)\n","      w_image[row_start:row_start + sq_size, col_start:col_start + sq_size,:] = work_array.copy()\n","      work_array = temp_array.copy()\n","    touched[next_no] = True\n","    cur_no = next_no\n","  return w_image"],"execution_count":0,"outputs":[]},{"metadata":{"id":"A5mfzzRQBEb7","colab_type":"text"},"cell_type":"markdown","source":["Process the Content Image with permutations, rotations and color shifts.  <i>(These aren't necessary, but I found the result more interesting.)</i>"]},{"metadata":{"id":"kYUX34nzBHjd","colab_type":"code","colab":{}},"cell_type":"code","source":["#Set \n","assert target_image_rows == target_image_cols, \"The target number of rows \" + str(target_image_rows) + \" must equal the number of columns \" + str(target_image_cols)\n","img_size = target_image_rows\n","\n","image_path = make_content_name(content_image_file)\n","num_sq = int(number_blocks)\n","t_size = 0\n","if( int(target_image_rows/num_sq) * num_sq == target_image_rows):\n","  t_size = target_image_rows\n","else:\n","  t_size = (int(target_image_rows/num_sq) + 1) * num_sq\n","\n","print(\"Target Content Image Size {:d}\".format(target_image_rows))\n","print(\"Working Image Size: {:d}\".format(t_size))\n","\n","out_path = make_content_name(stage_one_output_file)\n","\n","# matrix_0, matrix_90, matrix_180, matrix_270 = define_rotation_matrices(t_size, num_sq)\n","\n","\n","# content_image = \n","content_image = io.imread(image_path)\n","content_array = transform.resize(content_image,(t_size,t_size))\n","print(\"Content Image - Resampled\")\n","pprint(content_array.shape)\n","plt.imshow(content_array)\n","plt.show()\n","tot_sq = num_sq * num_sq\n","#permute image\n","#set this if you want to use a specific permutation\n","rand_perm_in = [8, 3, 1, 2, 7, 6, 0, 5, 4]\n","np.random.seed(0)\n","image_out = None\n","if do_permute:\n","  rand_perm = np.random.permutation(tot_sq) if random_permute else rand_perm_in\n","  pprint(\"The permutation\")\n","  pprint(rand_perm)\n","  image_out = permute_image(content_array,rand_perm,num_sq)\n","else:\n","  image_out = content_array.copy()\n","\n","plt.imshow(image_out)\n","plt.show()\n","\n","if do_shift:\n","  print(\"Blue Shift {:f}\".format(blue_shift))\n","  image_out[:,:,2] = image_out[:,:,2] + 2.0 * blue_shift\n","  image_out[:,:,0] = image_out[:,:,0] - blue_shift/3.0\n","  image_out[:,:,1] = image_out[:,:,1] - 2.0 * blue_shift/3.0\n","  #clip\n","  image_out = np.clip(image_out,0.0,1.0)\n","\n","plt.imshow(image_out)\n","plt.show()\n","\n","if do_rotation:\n","  nx,ny,m = image_out.shape\n","  sq_size = int(nx/num_sq)\n","  p_rotate = 1.0/8.\n","  move_matrix = None\n","  rot_val = 1\n","  np.random.seed(np.random.randint(0,high=np.iinfo(np.int32).max))\n","  for k in range(num_sq * num_sq):\n","    r_val = np.random.rand()\n","    row_start,col_start = compute_start_rc(k,num_sq, sq_size)\n","    rotate = False\n","    rot_angle = 0.0\n","    if r_val < 3 * p_rotate:\n","      rot_val = (rot_val + 1) %3\n","      rotate = True\n","    if rotate and (rot_val ==0):\n","      rot_angle = 90.\n","      rotate = True\n","    elif rotate and (rot_val == 1):\n","      rot_angle = 180.\n","      rotate = True\n","    elif rotate and (rot_val == 2):\n","      rot_angle = 270.\n","      rotate = True\n","    if rotate:\n","      w_array = copy_image_part(image_out, k, num_sq)\n","      t_array = np.zeros(w_array.shape)\n","      wx, wy, wm = w_array.shape\n","      t_array = transform.rotate(w_array,rot_angle)\n","      image_out[row_start:row_start+sq_size,col_start:col_start+ sq_size,:] = t_array\n","    \n","if do_permute:\n","  permute_array = [0, 3, 2, 7, 4, 5, 6, 1, 8]\n","  pprint(rand_perm)\n","  image_out = permute_image(image_out,permute_array,num_sq)\n","  \n","show_image = image_out * 255.\n","show_image = np.array(show_image).astype(int)\n","show_out = np.uint8(show_image)\n","plt.imshow(image_out)\n","plt.show()\n","!rm -rf out_path\n","io.imsave(out_path,show_out)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2NuFe4NGBmdL","colab_type":"text"},"cell_type":"markdown","source":["Check the files."]},{"metadata":{"id":"5BVVz6B0Bo4-","colab_type":"code","colab":{}},"cell_type":"code","source":["!ls -alF"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SsDBKWOvCRlb","colab_type":"text"},"cell_type":"markdown","source":["##3.  Use \"Gradient Descent\" for Neural Style Transfer\n","\n","In this process we read in the content image and style images to perform Neural Style Transfer.  The differnce in this part of the notebook is the way we use autograd.  The use of autograd within PyTorch is very powerful, namely, there is nothing restricting us to a single gradient descent optimization technique during the optimization process.  The PyTorch autograd method allows us some flexibility to both converge faster to a solution and smooth the solution as we approach the end of the process."]},{"metadata":{"id":"mK2hndnhG3he","colab_type":"text"},"cell_type":"markdown","source":["###3.1  Load the Images onto the GPU"]},{"metadata":{"id":"_qyS4sLlCYSx","colab_type":"code","colab":{}},"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#load the images\n","content_file = 'working_image_one.jpg'\n","style_file = 'picasso.jpg'\n","loader = transforms.Compose([\n","    transforms.Resize([img_size,img_size],PIL.Image.LANCZOS),  # scale imported image\n","    transforms.ToTensor()])\n","#\n","content_file = make_content_name(content_file)\n","style_file = make_content_name(style_file)\n","pprint(content_file)\n","pprint(style_file)\n","content_image = Image.open(content_file)\n","content_image = loader(content_image).to(device,torch.float)\n","style_image = Image.open(style_file)\n","style_image = loader(style_image).to(device,torch.float)\n","#\n","#  Build the target image\n","#\n","target_image = content_image.clone()\n","\n","pprint(type(content_image))\n","pprint(content_image.device)\n","pprint(content_image.shape)\n","pprint(type(style_image))\n","pprint(style_image.device)\n","pprint(style_image.shape)\n","pprint(type(target_image))\n","pprint(target_image.device)\n","pprint(target_image.shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OBMqmBllDrL1","colab_type":"text"},"cell_type":"markdown","source":["Show the images"]},{"metadata":{"id":"eEEu8XVaDtHA","colab_type":"code","colab":{}},"cell_type":"code","source":["unloader = transforms.ToPILImage()  # reconvert into PIL image\n","\n","def imshow(tensor, title=None):\n","    image = tensor.cpu().clone()  # we clone the tensor to not do changes on it\n","    image = image.squeeze(0)      # remove the fake batch dimension\n","    image = unloader(image)\n","    plt.imshow(image)\n","    if title is not None:\n","        plt.title(title)\n","    plt.pause(0.001) \n","    \n","plt.figure()\n","imshow(style_image, title='Style Image')\n","\n","plt.figure()\n","imshow(content_image, title='Content Image')\n","\n","plt.figure()\n","imshow(target_image, title='Target Image')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1tYJOK0RHKQM","colab_type":"text"},"cell_type":"markdown","source":["###3.2 Define the Functions that Generate a \"Loss\" Function"]},{"metadata":{"id":"MlDfm80AthHu","colab_type":"text"},"cell_type":"markdown","source":["This isn't a true PyTorch Loss function, but rather a measure of the way we want to converge the image between the content and style.  Autograd can work with this looser definition of a loss as easily as it works with an Autograd Loss function.\n","\n"]},{"metadata":{"id":"Q3ia8CwuthHx","colab_type":"text"},"cell_type":"markdown","source":["Loss Functions\n","--------------\n","Define the loss functions for the style and content\n","\n","\n"]},{"metadata":{"id":"hyqsUTKethHz","colab_type":"code","colab":{}},"cell_type":"code","source":["class ContentLoss(nn.Module):\n","\n","    def __init__(self, target,):\n","        super(ContentLoss, self).__init__()\n","        # we 'detach' the target content from the tree used\n","        # to dynamically compute the gradient: this is a stated value,\n","        # not a variable. Otherwise the forward method of the criterion\n","        # will throw an error.\n","        self.target = target.detach()\n","\n","    def forward(self, input):\n","        self.loss = F.mse_loss(input, self.target)\n","        return input\n","      \n","def gram_matrix(input):\n","    a, b, c, d = input.size()  # a=batch size(=1)\n","    # b=number of feature maps\n","    # (c,d)=dimensions of a f. map (N=c*d)\n","\n","    features = input.view(a * b, c * d)  # resise F_XL into \\hat F_XL\n","\n","    G = torch.mm(features, features.t())  # compute the gram product\n","\n","    # we 'normalize' the values of the gram matrix\n","    # by dividing by the number of element in each feature maps.\n","    return G.div(a * b * c * d)\n","  \n","class StyleLoss(nn.Module):\n","\n","    def __init__(self, target_feature):\n","        super(StyleLoss, self).__init__()\n","        self.target = gram_matrix(target_feature).detach()\n","\n","    def forward(self, input):\n","        G = gram_matrix(input)\n","        self.loss = F.mse_loss(G, self.target)\n","        return input\n","      \n","def content_and_style(style_losses, content_losses, style_weight, content_weight):\n","  style_score = 0.0\n","  content_score = 0\n","\n","  for sl in style_losses:\n","      style_score += sl.loss\n","  for cl in content_losses:\n","      content_score += cl.loss\n","\n","  style_score *= style_weight\n","  content_score *= content_weight\n","  return content_score, style_score"],"execution_count":0,"outputs":[]},{"metadata":{"id":"E1oejX4xthID","colab_type":"text"},"cell_type":"markdown","source":["## Building the Model\n","\n","\n","Now we need to import a pre-trained neural network. We will use a 19\n","layer VGG network like the one used in the paper.\n","\n","PyTorch’s implementation of VGG is a module divided into two child\n","``Sequential`` modules: ``features`` (containing convolution and pooling layers),\n","and ``classifier`` (containing fully connected layers). We will use the\n","``features`` module because we need the output of the individual\n","convolution layers to measure content and style loss. Some layers have\n","different behavior during training than evaluation, so we must set the\n","network to evaluation mode using ``.eval()``.\n","\n","\n"]},{"metadata":{"id":"GFxIJ0jFthIF","colab_type":"code","colab":{}},"cell_type":"code","source":["#Global normalization based upon pretrained model\n","cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n","cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n","# cnn_normalization_mean = torch.tensor([0.406, 0.456, 0.485]).to(device)\n","# cnn_normalization_std = torch.tensor([0.225, 0.224, 0.229]).to(device)\n","#Load the model\n","cnn = models.vgg19(pretrained=True).features.to(device).eval()\n","# create a module to normalize input image so we can easily put it in a\n","# nn.Sequential\n","class Normalization(nn.Module):\n","    def __init__(self, mean, std):\n","        super(Normalization, self).__init__()\n","        # .view the mean and std to make them [C x 1 x 1] so that they can\n","        # directly work with image Tensor of shape [B x C x H x W].\n","        # B is batch size. C is number of channels. H is height and W is width.\n","        self.mean = torch.tensor(mean).view(-1, 1, 1)\n","        self.std = torch.tensor(std).view(-1, 1, 1)\n","\n","    def forward(self, img):\n","        # normalize img\n","        return (img - self.mean) / self.std"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Sybl7iCythIO","colab_type":"text"},"cell_type":"markdown","source":["A ``Sequential`` module contains an ordered list of child modules. For\n","instance, ``vgg19.features`` contains a sequence (Conv2d, ReLU, MaxPool2d,\n","Conv2d, ReLU…) aligned in the right order of depth. We need to add our\n","content loss and style loss layers immediately after the convolution\n","layer they are detecting. To do this we must create a new ``Sequential``\n","module that has content loss and style loss modules correctly inserted.\n","\n","\n"]},{"metadata":{"id":"zTehFhYqthIP","colab_type":"code","colab":{}},"cell_type":"code","source":["# desired depth layers to compute style/content losses :\n","content_layers_default = ['conv2_3_2']\n","style_layers_default = [ 'conv2_3_1', 'conv2_4_2', 'conv2_5_3']\n","\n","def get_style_model_and_losses(in_cnn, norm_mean, norm_std,\n","                               style_img, content_img,\n","                               content_layers=content_layers_default,\n","                               style_layers=style_layers_default):\n","  \n","  cnn = copy.deepcopy(in_cnn)\n","  normalization = Normalization(norm_mean, norm_std).to(accelerator)\n","  #Define list for content_losses and style_losses\n","  content_losses = []\n","  style_losses = []\n","  #Create new model\n","  model = nn.Sequential(normalization)\n","  name = ''\n","  l = 1\n","  conv_sub_l = 1\n","  relu_sub_l = 1\n","  for layer in cnn.children():\n","    if isinstance(layer, nn.Conv2d):\n","      name = 'conv2_{:d}_{:d}'.format(l,conv_sub_l)\n","      conv_sub_l += 1\n","    elif isinstance(layer, nn.ReLU):\n","      name = 'relu_{:d}_{:d}'.format(l, relu_sub_l)\n","      layer = nn.ReLU(inplace=False)\n","      relu_sub_l += 1\n","    elif isinstance(layer, nn.MaxPool2d):\n","      name = 'pool_{:d}'.format(l)\n","      l += 1\n","      conv_sub_l = 1\n","      relu_sub_l = 1\n","    else:\n","      raise RuntimeError('Unknown Layer: {}'.format(layer.__class__.__name__))\n","    #\n","    model.add_module(name, layer)\n","    #\n","    if name in content_layers:\n","      target = model(content_img).detach()\n","      content_loss = ContentLoss(target)\n","      model.add_module(\"content_loss_{:s}\".format(name), content_loss)\n","      content_losses.append(content_loss)\n","    #\n","    if name in style_layers:\n","      # add style loss:\n","      target_feature = model(style_img).detach()\n","      style_loss = StyleLoss(target_feature)\n","      model.add_module(\"style_loss_{:s}\".format(name), style_loss)\n","      style_losses.append(style_loss)\n","\n","#   for layer in model.named_children():\n","#     pprint(layer[0])\n","\n","\n","  return model, style_losses, content_losses"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1UJMxdGVthIW","colab_type":"text"},"cell_type":"markdown","source":["## Define Gradient Descent\n","\n","This uses the Adam descent algorithm\n","\n","\n"]},{"metadata":{"id":"0MMhePYXthIa","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_input_optimizer0(input_img):\n","    # this line to show that input is a parameter that requires a gradient\n","    optimizer = optim.Adam([input_img.requires_grad_()], lr=.01)\n","    return optimizer\n","  \n","def get_input_optimizer1(input_img):\n","    # this line to show that input is a parameter that requires a gradient\n","    optimizer = optim.Adam([input_img.requires_grad_()], lr=.005)\n","    return optimizer\n","  \n","def get_input_optimizer2(input_img):\n","    # this line to show that input is a parameter that requires a gradient\n","    optimizer = optim.Adam([input_img.requires_grad_()], lr=.001)\n","    return optimizer"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2O6OkpetthIe","colab_type":"text"},"cell_type":"markdown","source":["Finally, we must define a function that performs the neural transfer. For\n","each iteration of the networks, it is fed an updated input and computes\n","new losses. We will run the ``backward`` methods of each loss module to\n","dynamicaly compute their gradients. The optimizer requires a “closure”\n","function, which reevaluates the modul and returns the loss.\n","\n","We still have one final constraint to address. The network may try to\n","optimize the input with values that exceed the 0 to 1 tensor range for\n","the image. We can address this by correcting the input values to be\n","between 0 to 1 each time the network is run.\n","\n","\n"]},{"metadata":{"id":"ixbCIPllthIg","colab_type":"code","colab":{}},"cell_type":"code","source":["global last_loss, cur_opt, run_step, cur_rel_err\n","last_loss = 0.0\n","cur_opt = None\n","cur_rel_err = 1.0\n","run_step = -1\n","def run_style_transfer(cnn, normalization_mean, normalization_std,\n","                       content_img, style_img, input_img, num_steps=10000,\n","                       style_weight=1000000, content_weight=1):\n","  global last_loss, cur_opt, run_step, cur_rel_err\n","  \"\"\"Run the style transfer.\"\"\"\n","  print('Building the style transfer model..')\n","  model, style_losses, content_losses = get_style_model_and_losses(cnn,\n","                                                                   normalization_mean, normalization_std, style_img, content_img)\n","  cur_opt = get_input_optimizer1(input_img)\n","  print(cur_opt)\n","  print(type(cur_opt))\n","  cur_rel_err = 0.5\n","  print('Optimizing..')\n","  run_step = 0\n","  while run_step <= num_steps:\n","    def closure():\n","      global last_loss, cur_opt, run_step, cur_rel_err\n","      # correct the values of updated input image\n","      input_img.data.clamp_(0, 1)\n","      cur_opt.zero_grad()\n","      model(input_img)\n","      content_score, style_score = content_and_style(style_losses, content_losses, style_weight, content_weight)\n","      loss = content_score + style_score\n","      loss.backward()\n","      #This is included to demonstrate changing the gradient algorithm\n","      if run_step % 50 == 0:\n","        curr_loss = loss.item()\n","        cur_rel_err = np.abs((last_loss - curr_loss))\n","        last_loss = curr_loss\n","      if run_step % 500 == 0:\n","        print(\"step {:8d}:\".format(run_step))\n","        print('Style Loss : {:4f} Content Loss: {:4f}'.format(\n","            style_score.item(), content_score.item()))\n","        print()\n","        plt.figure()\n","        imshow(input_img, title='Output Image')\n","        plt.show\n","      return style_score + content_score\n","    #\n","    cur_opt.step(closure)\n","    if run_step % 1000 == 0:\n","      if cur_rel_err > 2.0:\n","        cur_opt = get_input_optimizer2(input_img)\n","      elif cur_rel_err > 1.0:\n","        cur_opt = get_input_optimizer1(input_img)\n","      else:\n","        cur_opt = get_input_optimizer0(input_img)\n","      pprint(cur_opt)\n","    run_step = run_step + 1\n","\n","  # a last correction...\n","  input_img.data.clamp_(0, 1)\n","\n","  return input_img, run_step"],"execution_count":0,"outputs":[]},{"metadata":{"id":"epzkedVfthIk","colab_type":"text"},"cell_type":"markdown","source":["Finally, we can run the algorithm.\n","\n","\n"]},{"metadata":{"id":"oP8idZVzthIm","colab_type":"code","colab":{}},"cell_type":"code","source":["content_image_s = torch.stack([content_image])\n","style_image_s = torch.stack([style_image])\n","target_image_s = content_image_s.clone()\n","output, N = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,\n","                            content_image_s, style_image_s, target_image_s)\n","print(\"Number of steps: {}\".format(N))\n","plt.figure()\n","imshow(output, title='Output Image')\n","\n","# sphinx_gallery_thumbnail_number = 4\n","plt.ioff()\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"t7LfSQ--KtQT","colab_type":"text"},"cell_type":"markdown","source":["##4.  Save the Results\n","\n","This is an easy step when using the colab_gdrive library.  We save the file locally, then transfer the file to the Google Drive."]},{"metadata":{"id":"khpXt0hRGwpj","colab_type":"text"},"cell_type":"markdown","source":["Copy the file to local storage"]},{"metadata":{"id":"KP1tKlpzET6Q","colab_type":"code","colab":{}},"cell_type":"code","source":["#\n","#  Save file locally\n","#\n","out_jpg_file = \"style_converged.jpg\"\n","out_jpg_full = make_content_name(out_jpg_file)\n","torchvision.utils.save_image(output.squeeze(),out_jpg_full)\n","!ls -alF\n","#\n","#  show the file\n","#\n","simple_loader = transforms.Compose([\n","    transforms.ToTensor()])\n","#\n","converge_image = Image.open(out_jpg_full)\n","converge_image = loader(converge_image).to(device,torch.float)\n","imshow(converge_image,title=\"Converged Image\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IbYNkdJKGtm1","colab_type":"text"},"cell_type":"markdown","source":["Copy the file to the google drive\n","\n","Recall that we have already set the current working directory with the chdir command, so this is relatively easy."]},{"metadata":{"id":"qyAaxeIhHdTw","colab_type":"code","colab":{}},"cell_type":"code","source":["print(myGdrive.getcwd())\n","print(os.getcwd())\n","myGdrive.copy_to(out_jpg_file) #This only supports copying from the local cwd"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QoOYLEKUSP-e","colab_type":"text"},"cell_type":"markdown","source":["## 5.  Conclusions\n","\n","This notebook demonstrated these important ideas:\n","1.  Using Colaboratory and Google Drive together to solve problems\n","2.  Installing and using Pytorch\n","3.  Using gradient methods within PyTorch while leveraging and extending previously trained models\n","\n","If you have any questions about the operation or contents of this notebook, please contact dr.david.race@gmail.com"]}]}